<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>construtor_datasets API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>construtor_datasets</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># ----------------------------------------------------------------
# Funções para construção de datasets, com base nos documentos
# persistidos no banco, para tarefas de PLN.
# ----------------------------------------------------------------
import spacy
import os
import pickle
import numpy as np
import src.classes.persistencia.serializacao as ser
from unicodedata import normalize
from src.classes.persistencia.cliente import ClienteGenerico
from src.ambiente.parametros_globais import PERMISSION_ERROR, SPACY_MODEL_NOT_FOUND_ERROR, \
    FILE_NOT_FOUND_ERROR, INVALID_CONTENT
from src.ml.classificador import carregar_modelo, tratar_strings


def obter_metadados_documentos(c_mongo_meta, codproc, reprocessar=False):
    &#34;&#34;&#34;
    Obtém os metadados dos documentos que precisam ser processados
        
        @param c_mongo_meta: Base de dados onde estão os metadados dos documentos
        @param codproc: Código de processamento para realização do filtro na busca
        @param reprocessar: Indica se deve fazer o reprocessamento dos documentos, caso já tenha sido gerado o dataset
        @return: Metadados dos documentos que ainda não foram processados. Se for para reprocessar, retorna todos os
                 que atendam ao filtro &#39;codproc&#39;
    &#34;&#34;&#34;
    # Obtém os metadados dos documentos que serão processados
    if not reprocessar:
        # Traz todos os documentos que não tem o &#39;codproc&#39;
        doc_metadados_js = c_mongo_meta.buscar_todos(&#39;col_metadados_docs&#39;, &#39;Documento__cod_processamento&#39;,
                                                     f&#34;^((?!{codproc}).)*$&#34;)
    else:
        doc_metadados_js = c_mongo_meta.buscar_todos(&#39;col_metadados_docs&#39;, &#39;Documento__cod_processamento&#39;, codproc)

    return list(doc_metadados_js)


def obter_subsecoes_documentos(doc_metadados_js):
    &#34;&#34;&#34;
    Obtém as subseções dos documentos que precisam ser processados para criação do dataset

        @param doc_metadados_js: Metadados dos documentos que serão processados
        @return: Dicionário cuja chave é o código do arquivo e o valor é uma lista contendo as subseções do documento
    &#34;&#34;&#34;
    print(f&#34;\n=&gt; Etapa: Obtendo subseções...&#34;, end=&#39;&#39;, flush=True)

    c_mongo_doc = ClienteGenerico(&#39;MongoDB&#39;, &#39;localhost&#39;, 27017, &#39;db_documentos&#39;)
    documentos_subsecoes = {}  # A chave é o código do arquivo e o valor é uma lista contendo as subseções do documento

    # Obtém os documentos (através dos metadados) que serão processados
    for doc_meta in doc_metadados_js:
        doc = c_mongo_doc.buscar_um(&#39;col_editais&#39;, &#39;Edital__codigo_arq&#39;, doc_meta[&#39;Documento__codigo_arq&#39;])
        subsecoes = []

        if doc:
            doc_desserializado = ser.desserializar(doc)

            for s in doc_desserializado.secoes:
                for sb in s.subsecoes:
                    subsecoes.append(sb.descricao)

            documentos_subsecoes[doc_meta[&#39;Documento__codigo_arq&#39;]] = subsecoes

    c_mongo_doc.fechar_conexao()
    print(&#34; -&gt; Pronto!&#34;)
    return documentos_subsecoes


def classificar_subsecoes(documentos_subsecoes, filtro_retorno=None):
    &#34;&#34;&#34;
    Classifica as subseções em dois tipos: Jurídica (JUR) ou técnica (TEC)

        @param documentos_subsecoes: Dicionário contendo os documentos como chave e uma lista se subseções como valor
        @param filtro_retorno: Filtra o retorno da função por um rótulo específico
        @return: nome do modelo e (Se filtro_retorno for None, dicionário com o código do arquivo como chave e como
                 valor lista de tuplas contendo a subseção e a classificação dela, caso contrário, dicionário com o
                 código do arquivo como chave e como valor lista com as subseções classificadas e filtradas)
    &#34;&#34;&#34;
    documentos_subsecoes_classificadas = {}
    remover_stop_words = True

    # Escolhe o modelo de classificador para classificar as seções
    modelo = carregar_modelo(&#39;RandomForestClassifier&#39;, &#39;modelos&#39;)

    print(&#34;\n=&gt; Etapa: Classificar subseções.\n&#34;)
    total = len(documentos_subsecoes)
    cont = 1
    qtd_caracteres_limpar = 0  # Ajuda na limpeza dos códigos dos arquivos que já foram impressos na tela

    for cod_arq, subsecoes in documentos_subsecoes.items():
        print(f&#34; - Arquivo: {cont}/{total} =&gt; {cod_arq + qtd_caracteres_limpar * &#39; &#39;}&#34;, end=&#39;\r&#39;, flush=True)

        # Garante que não vai ficar lixo na tela, pois no mínimo vai limpar o tamanho do último código de arquivo
        qtd_caracteres_limpar = len(cod_arq)

        # Prepara as subseções e faz a predição dos rótulos
        subsecoes_aux = np.array(subsecoes)
        subsecoes_aux = subsecoes_aux.reshape((len(subsecoes_aux), 1))
        subsecoes_aux = tratar_strings(subsecoes_aux, remover_stop_words=remover_stop_words)
        rotulos = modelo.predict(subsecoes_aux)

        if rotulos:
            subsecoes_classificadas = []

            if filtro_retorno:
                for i in range(len(subsecoes)):
                    if rotulos[i] == filtro_retorno:
                        subsecoes_classificadas.append(subsecoes[i])
            else:
                for i in range(len(subsecoes)):
                    subsecoes_classificadas.append((subsecoes[i], rotulos[i]))

            documentos_subsecoes_classificadas[cod_arq] = subsecoes_classificadas

        cont += 1
    print(&#34;\n -&gt; Pronto!&#34;)
    return modelo.nome, documentos_subsecoes_classificadas


def dividir_em_sentencas(documentos_subsecoes, tamanho_minimo_sentenca):
    &#34;&#34;&#34;
    Divide as subseções em sentenças

        @param documentos_subsecoes: Dicionário cuja chave é o código do arquivo e o valor lista de subseções que serão
                                     divididas em sentenças
        @param tamanho_minimo_sentenca: Sentenças com tamanho menor que este parâmetro serão descartadas
        @return: Dicionário cuja chave é o código do arquivo e o valor é uma lista de sentenças
    &#34;&#34;&#34;
    print(&#34;\n=&gt; Etapa: Dividir em sentenças.\n&#34;)

    nlp = None

    modelo_spacy = &#39;pt_core_news_lg&#39;
    print(f&#34; - Carregando o modelo &#39;{modelo_spacy}&#39; do Spacy... &#34;, end=&#39;&#39;, flush=True)

    try:
        nlp = spacy.load(modelo_spacy)
    except OSError:
        print(f&#34;\n\n    ERRO: Não foi possível encontrar o modelo &#39;{modelo_spacy}&#39;. Confira o nome do modelo. Caso &#34;
              f&#34;esteja correto, faça o download dele utilizando o comando &#39;python -m spacy download {modelo_spacy}&#39; num &#34;
              f&#34;terminal.\n&#34;)
        exit(SPACY_MODEL_NOT_FOUND_ERROR)

    print(&#34;-&gt; Pronto!&#34;)

    documentos_sentencas = {}
    cont = 1
    total = len(documentos_subsecoes)

    print(&#34;\n - Preparando para gerar os arquivos:\n&#34;)

    for cod_arq, subsecoes in documentos_subsecoes.items():
        sentencas = []
        print(f&#34;   Documento ({cont}/{total}): {cod_arq} &#34;, end=&#39;&#39;, flush=True)

        for sub in subsecoes:
            nlp_doc = nlp(sub)

            for sent in nlp_doc.sents:
                lst_tokens = [t.text for t in sent]

                if len(lst_tokens) &gt;= tamanho_minimo_sentenca:
                    sentencas.append(&#34; &#34;.join(lst_tokens))

        documentos_sentencas[cod_arq] = sentencas
        print(&#34;-&gt; OK&#34;)
        cont += 1

    return documentos_sentencas


def marcar_documentos_processados(c_mongo_meta, doc_metadados_js, codproc, reprocessado=False):
    &#34;&#34;&#34;
    Marca os documentos que foram processados

        @param c_mongo_meta: Base de dados onde estão os metadados dos documentos
        @param doc_metadados_js: Lista com os documentos que foram processados
        @param codproc: Código de processamento
        @param reprocessado: Indica se foi feito o reprocessamento dos documentos
        @return: Lista com códigos de arquivos que tiveram erro na marcação. Se tudo ocorrer bem, esta lista será vazia
    &#34;&#34;&#34;
    doc_erros = []  # Guarda os códigos dos documentos caso haja erro na marcação do processamento

    # Se não for reprocessamento, atualiza os códigos de processamento. Se for reprocessamento, não atualiza a lista,
    # pois o código já se encontra nela.
    if not reprocessado:
        for doc_meta in doc_metadados_js:
            doc_cod_processamento = doc_meta[&#39;Documento__cod_processamento&#39;]

            # Se o documento ainda não foi processado com algum código de processamento, retira o código &#39;a processar&#39;
            # e grava o &#39;codproc&#39;
            if &#39;a processar&#39; in doc_cod_processamento:
                doc_cod_processamento = codproc
            else:  # Se já foi processado com algum código, acrescenta o &#39;codproc&#39; na string de códigos de processamento
                doc_cod_processamento += f&#34;, {codproc}&#34;

            resultado = c_mongo_meta.alterar(&#39;col_metadados_docs&#39;, &#39;Documento__codigo_arq&#39;,
                                             doc_meta[&#39;Documento__codigo_arq&#39;], &#39;Documento__cod_processamento&#39;,
                                             doc_cod_processamento)

            if not resultado:
                doc_erros.append(doc_meta[&#39;Documento__codigo_arq&#39;])

    return doc_erros


def salvar_sentencas_base(caminho, nome_arquivo, sentencas_base):
    &#34;&#34;&#34;
    Salva as sentenças base para auxiliar na verificação de documentos futuros

        @param caminho: Caminho onde as sentenças base serão salvas
        @param nome_arquivo: Nome do arquivo onde as sentenças base serão salvas
        @param sentencas_base: Lista contendo as sentenças base que serão salvas
    &#34;&#34;&#34;
    arq = None
    caminho_arq_sentencas_base = os.path.join(caminho, nome_arquivo)

    try:
        arq = open(caminho_arq_sentencas_base, &#39;wb&#39;)
    except FileNotFoundError:
        print(f&#34;\nErro ao salvar as sentenças base. O caminho &#39;{caminho}&#39; não existe!\n&#34;)
        exit(FILE_NOT_FOUND_ERROR)
    except PermissionError:
        print(f&#34;\nErro ao salvar as sentenças base no caminho &#39;{caminho}&#39;. Permissão de escrita negada!\n&#34;)
        exit(PERMISSION_ERROR)

    pickle.dump(sentencas_base, arq)
    arq.close()


def carregar_sentencas_base(caminho, nome_arquivo):
    &#34;&#34;&#34;
    Carrega as sentenças base para auxiliar na verificação de documentos

        @param caminho: Caminho onde as sentenças base foram salvas
        @param nome_arquivo: Nome do arquivo contendo as sentenças base
        @return: Lista contendo as sentenças base que foram carregadas, ou, caso o arquivo não exista, uma lista vazia
    &#34;&#34;&#34;
    arq = None
    caminho_arq_sentencas_base = os.path.join(caminho, nome_arquivo)

    try:
        arq = open(caminho_arq_sentencas_base, &#39;rb&#39;)
    except FileNotFoundError:
        print(f&#34;\nErro ao carregar as sentenças base. O arquivo &#39;{caminho_arq_sentencas_base}&#39; não foi encontrado! &#34;
              f&#34;Retornando uma base vazia...\n&#34;)
        return []
    except PermissionError:
        print(f&#34;\nErro ao carregar as sentenças base do caminho &#39;{caminho_arq_sentencas_base}&#39;. Permissão de leitura &#34;
              f&#34;negada!\n&#34;)
        exit(PERMISSION_ERROR)

    sentencas_base = pickle.load(arq)
    arq.close()
    return sentencas_base


def retirar_sentencas_similares(documentos_sentencas, caminho_sentencas_base, nome_arq_sents_base, caminho_relatorio,
                                reprocessar=False, escopo_global_sentencas=True, limiar=0.90):
    &#34;&#34;&#34;
    Retira sentenças similares de documentos, com base no Índice de Jaccart e um limiar.

        @param documentos_sentencas: Dicionário onde a chave é o código do arquivo e o valor é uma lista de sentenças
        @param caminho_sentencas_base: Caminho para obtenção e/ou gravação das sentenças base
        @param nome_arq_sents_base: Nome do arquivo de sentenças base
        @param caminho_relatorio: Caminho onde será gerado o relatório com os detalhes das sentenças descartadas
        @param reprocessar: Indica se deve fazer o reprocessamento dos documentos, caso já tenha sido construido um
                            dataset antes
        @param escopo_global_sentencas: Escopo para a análise de sentenças. Se True, fará a comparação com todos os
                                        documentos, caso contrário a comparação será somente no próprio documento
        @param limiar: Limiar para o descarte. Caso o Índice de Jaccart fique acima do limiar a sentença é descartada
        @return: documentos_sentencas sem as sentenças similares, se houverem
    &#34;&#34;&#34;
    print(f&#34;\n=&gt; Etapa: Retirando sentenças similares (escopo_global_sentencas={escopo_global_sentencas}; &#34;
          f&#34;limiar={limiar}).\n&#34;)

    # Guarda informações das sentenças de todos os documentos para avaliação da similaridade.
    # Estrutura de cada sentença base da lista: (&#34;nome do arquivo&#34;, sentença, set da sentença tratada). Exemplo:
    # (&#34;Edital_001.pdf&#34;, &#34;O índice de reajuste não foi o maior de 2021&#34;,
    # {&#39;2021&#39;, &#39;de&#39;, &#39;foi&#39;, &#39;maior&#39;, &#39;nao&#39;, &#39;o&#39;, &#39;reajuste&#39;, &#39;indice&#39;})
    # Obs.: Não carrega o arquivo de sentenças base se for reprocessar, pois se carregar, todas as sentenças serão
    # descartadas!
    if escopo_global_sentencas and not reprocessar:
        sentencas_base = carregar_sentencas_base(caminho_sentencas_base, nome_arq_sents_base)
    else:
        sentencas_base = []

    cont = 1
    qtd_documentos = len(documentos_sentencas)
    sentencas_descartadas_por_arquivo = {}
    total_global_sentencas = 0
    total_global_sentencas_descartadas = 0

    for cod_arq, sentencas in documentos_sentencas.items():
        if not escopo_global_sentencas:
            sentencas_base = []

        print(f&#34;   Documento ({cont}/{qtd_documentos}): {cod_arq} &#34;, end=&#39;&#39;, flush=True)

        sentencas_a_inserir = []
        sentencas_descartadas = []

        for sent_aval in sentencas:
            tipo_sent_aval = type(sent_aval)

            if tipo_sent_aval is not str and tipo_sent_aval is not tuple:
                print(f&#34;\n\nO Formato &#39;{tipo_sent_aval}&#39; não é aceito para as sentenças. Sentença errada: &#39;{sent_aval}&#39;&#34;
                      f&#34;. Formatos aceitos: &#39;str&#39; e &#39;tuple&#39;.\n&#34;)
                exit(INVALID_CONTENT)

            sent_inserir = sent_aval

            # Trata o caso das sentenças no formato JSON vindas da rotina para conversão dos arquivos anotados no
            # Doccano para o formato CONLL. Neste caso, sent_aval será uma tupla contendo na posição 0 o número da linha
            # da sentença no arquivo de sentenças anotadas e na posição 1 a sentença no formato JSON
            if tipo_sent_aval is tuple:
                sent_aval = sent_aval[1][&#39;data&#39;]

            if len(sent_aval) == 0:
                continue

            # Retira acentos, cedilhas, etc... (transforma em codificação ASCII) para considerar palavras acentuadas
            # corretamente iguais a outras que não foram
            sent_aval_aux = normalize(&#39;NFKD&#39;, sent_aval).encode(&#39;ASCII&#39;, &#39;ignore&#39;).decode(&#39;ASCII&#39;)

            sent_aval_aux = set(sent_aval_aux.lower().split())
            inserir = True

            # Estrutura da sent_base do for abaixo: (&#34;nome do arquivo&#34;, sentença, set da sentença tratada). Exemplo:
            # (&#34;Edital_001.pdf&#34;, &#34;O índice de reajuste não foi o maior de 2021&#34;,
            # {&#39;2021&#39;, &#39;de&#39;, &#39;foi&#39;, &#39;maior&#39;, &#39;nao&#39;, &#39;o&#39;, &#39;reajuste&#39;, &#39;indice&#39;})

            sentenca_semelhante = None
            ind_jaccart_sentenca_descartada = None

            for sent_base in sentencas_base:
                ind_jaccart = len(sent_base[2].intersection(sent_aval_aux)) / len(sent_base[2].union(sent_aval_aux))

                if ind_jaccart &gt; limiar:
                    inserir = False
                    sentenca_semelhante = sent_base
                    ind_jaccart_sentenca_descartada = ind_jaccart
                    break

            if inserir:
                sentencas_base.append((cod_arq, sent_aval, sent_aval_aux))
                sentencas_a_inserir.append(sent_inserir)
            else:
                sentencas_descartadas.append((sent_aval, sent_aval_aux, ind_jaccart_sentenca_descartada,
                                              sentenca_semelhante))

        total_sentencas = len(sentencas)
        qtd_sentencas_descartadas = len(sentencas_descartadas)
        total_global_sentencas += total_sentencas
        total_global_sentencas_descartadas += qtd_sentencas_descartadas

        print(f&#34;(Sentenças descartadas/total: {qtd_sentencas_descartadas}/{total_sentencas} - &#34;
              f&#34;{(qtd_sentencas_descartadas / total_sentencas) * 100:.2f}%)-&gt; OK&#34;)

        documentos_sentencas[cod_arq] = sentencas_a_inserir
        sentencas_descartadas_por_arquivo[cod_arq] = (sentencas_descartadas, total_sentencas)

        cont += 1

    total_sentencas_aproveitadas = total_global_sentencas - total_global_sentencas_descartadas

    if total_global_sentencas &gt; 0:
        print(f&#34;\n# Resultado: Das {total_global_sentencas} sentenças encontradas nos {qtd_documentos} documentos, &#34;
              f&#34;foram aproveitadas {total_sentencas_aproveitadas} &#34;
              f&#34;({(total_sentencas_aproveitadas / total_global_sentencas) * 100:.2f}%) e descartadas &#34;
              f&#34;{total_global_sentencas_descartadas} &#34;
              f&#34;({(total_global_sentencas_descartadas / total_global_sentencas) * 100:.2f}%)\n&#34;)

        # Gera um arquivo com o detalhamento das sentenças descartadas
        caminho_arq_detalhes_descarte = os.path.join(caminho_relatorio, &#34;detalhamento_sentencas_descartadas.txt&#34;)

        with open(caminho_arq_detalhes_descarte, &#34;w&#34;) as arq:
            for cod_arq, sent_descartadas in sentencas_descartadas_por_arquivo.items():
                qtd_sentencas_descartadas = len(sent_descartadas[0])
                qtd_total_sentencas = sent_descartadas[1]

                arq.write(f&#34;# Documento: {cod_arq} - Qtd. sentenças descartadas/Qtd. sentenças: &#34;
                          f&#34;{qtd_sentencas_descartadas}/{qtd_total_sentencas} &#34;
                          f&#34;({(qtd_sentencas_descartadas / qtd_total_sentencas) * 100:.2f}%)\n\n&#34;)

                for sent_descartada in sent_descartadas[0]:
                    arq.write(f&#34;   Sentença descartada...............: {sent_descartada[0]}\n&#34;)
                    arq.write(f&#34;   Set sent. descartada..............: {sent_descartada[1]}\n&#34;)
                    arq.write(f&#34;   Índice de Jaccart.................: {sent_descartada[2]}\n&#34;)
                    arq.write(&#34;   &gt; Dados da sentença base semelhante\n&#34;)
                    arq.write(f&#34;                - Nome do documento..: {sent_descartada[3][0]}\n&#34;)
                    arq.write(f&#34;                - Sentença base......: {sent_descartada[3][1]}\n&#34;)
                    arq.write(f&#34;                - Set sent. base.....: {sent_descartada[3][2]}\n\n&#34;)

                arq.write(&#34;\n\n&#34;)

    if escopo_global_sentencas:
        salvar_sentencas_base(caminho_sentencas_base, nome_arq_sents_base, sentencas_base)

    return documentos_sentencas


def construir_dataset(data_hora, codproc, caminho, caminho_sentencas_base, tamanho_minimo_sentenca, qtd_max_sent=5,
                      reprocessar=False, organizar_em_pastas=False, cabecalho=False,
                      retirar_sentencas_semelhantes=False, escopo_global_sentencas=True, limiar=0.90):
    &#34;&#34;&#34;
    Constrói um dataset de acordo com o tipo de processamento e grava em arquivo(s)

        @param data_hora: Data e hora para compor o nome da pasta onde os datasets serão gerados
        @param codproc: Código do processamento a ser realizado (ner ou classificacao)
        @param caminho: Caminho onde o(s) arquivo(s) será(ão) gerados
        @param caminho_sentencas_base: Caminho para obtenção e/ou gravação das sentenças base
        @param tamanho_minimo_sentenca: Sentenças com tamanho menor que este parâmetro serão descartadas
        @param qtd_max_sent: Quantidade máxima de sentenças em cada arquivo. Sem efeito se o parâmetro
                             &#39;organizar_em_pastas&#39; for False
        @param reprocessar: Indica se deve fazer o reprocessamento dos documentos, caso já tenha sido construido um
                            dataset antes
        @param organizar_em_pastas: Indica se os editais serão organizados em pasta (uma pasta para cada edital)
        @param cabecalho: Se deve ou não inserir cabeçalho nos arquivos de dataset para classificação
        @param retirar_sentencas_semelhantes: Indica se as sentenças semelhantes devem ser retiradas no momento de
                                              geração dos datasets
        @param escopo_global_sentencas: Escopo para a análise de sentenças. Se True, fará a comparação com todos os
                                        documentos, caso contrário a comparação será somente no próprio documento
        @param limiar: Limiar para o descarte. Caso o Índice de Jaccart fique acima do limiar a sentença é descartada
        @return: Dicionário com os datasets construidos e caminho onde eles foram salvos
    &#34;&#34;&#34;
    documentos_sentencas = {}  # A chave é o código do arquivo e o valor é uma lista de sentenças
    caminho_com_data = &#34;&#34;  # Guarda o caminho onde os datasets serão gerados

    c_mongo_meta = ClienteGenerico(&#39;MongoDB&#39;, &#39;localhost&#39;, 27017, &#39;db_metadados&#39;)
    doc_metadados_js = obter_metadados_documentos(c_mongo_meta, codproc, reprocessar)

    if doc_metadados_js:
        print(&#34;\n                 #------- Processando documentos -------#\n&#34;)

        # Ajuda a organizar a pasta de datasets
        caminho_com_data = os.path.join(caminho, f&#34;{codproc}_{data_hora}&#34;)

        documentos_subsecoes = obter_subsecoes_documentos(doc_metadados_js)
        tam_documentos_subsecoes = len(documentos_subsecoes)
        lst_documentos = list(documentos_subsecoes.keys())
        processado = False

        # Guardam esses dados para informar no arquivo de parâmetros utilizados
        filtro = &#39;&#39;
        nome_modelo = &#39;&#39;

        # Cria uma pasta para armazenar os datasets
        try:
            os.makedirs(caminho_com_data, exist_ok=True)
        except PermissionError:
            print(
                f&#34;\nErro ao criar a pasta &#39;{caminho_com_data}&#39; para guardar os datasets. &#34;
                f&#34;Permissão de escrita negada!\n&#34;)
            exit(PERMISSION_ERROR)

        if codproc == &#39;ner&#39;:
            filtro = &#39;JUR&#39;
            nome_modelo, documentos_subsecoes_classificadas = classificar_subsecoes(documentos_subsecoes,
                                                                                    filtro_retorno=filtro)
            del documentos_subsecoes
            documentos_sentencas = dividir_em_sentencas(documentos_subsecoes_classificadas, tamanho_minimo_sentenca)
            del documentos_subsecoes_classificadas

            if retirar_sentencas_semelhantes:
                documentos_sentencas = \
                    retirar_sentencas_similares(documentos_sentencas, caminho_sentencas_base, &#34;SentencasBaseNER.pkl&#34;,
                                                caminho_com_data, reprocessar=reprocessar,
                                                escopo_global_sentencas=escopo_global_sentencas, limiar=limiar)
            arq_dataset = None

            print(&#34;\n=&gt; Etapa: Gerando arquivos.\n&#34;)

            if organizar_em_pastas:
                # Trata a quantidade máxima de sentenças para evitar problemas
                if qtd_max_sent &lt; 5:
                    qtd_max_sent = 5

                for cod_arq, sentencas in documentos_sentencas.items():
                    nome_pasta = cod_arq.split(&#39;.&#39;)[0]  # Retira a extensão do código do arquivo
                    nome_arquivo = &#39;_&#39;.join(nome_pasta.split(&#39;_&#39;)[1:])  # Retira a prefixo numérico único

                    if len(sentencas) == 0:
                        nome_pasta = &#34;DOCUMENTO_SEM_SENTENÇAS_ÚNICAS_&#34; + nome_pasta

                    # Cria uma pasta para cada documento (sem a extensão (ex: .pdf) do código do arquivo)
                    pasta_destino_dataset = os.path.join(caminho_com_data, nome_pasta)

                    try:
                        os.makedirs(pasta_destino_dataset, exist_ok=True)
                    except PermissionError:
                        print(
                            f&#34;\nErro ao criar a pasta &#39;{pasta_destino_dataset}&#39; para guardar os datasets. &#34;
                            f&#34;Permissão de escrita negada!\n&#34;)
                        exit(PERMISSION_ERROR)

                    seq = 1  # Sequência numérica para compor os nomes dos datasets

                    for i in range(len(sentencas)):
                        # Gera um arquivo diferente a cada &#39;qtd_max_sent&#39; sentenças
                        if (i % qtd_max_sent) == 0:
                            if arq_dataset:
                                arq_dataset.close()

                            caminho_destino_dataset = os.path.join(pasta_destino_dataset, f&#34;{nome_arquivo}_{seq}.txt&#34;)

                            try:
                                arq_dataset = open(caminho_destino_dataset, &#39;w&#39;)
                            except PermissionError:
                                print(f&#34;\nErro ao criar o arquivo de dataset na pasta &#39;{pasta_destino_dataset}&#39;. &#34;
                                      f&#34;Permissão de gravação negada!\n&#34;)
                                exit(PERMISSION_ERROR)

                            seq += 1

                        arq_dataset.write(f&#34;{sentencas[i]}\n&#34;)
            else:
                # Não aplicável quando os documentos não forem organizados por pasta
                qtd_max_sent = &#39;n/a&#39;

                for cod_arq, sentencas in documentos_sentencas.items():
                    nome_arquivo = cod_arq.split(&#39;.&#39;)[0] + &#39;.txt&#39;
                    caminho_destino_dataset = os.path.join(caminho_com_data, nome_arquivo)

                    try:
                        arq_dataset = open(caminho_destino_dataset, &#39;w&#39;)
                    except PermissionError:
                        print(f&#34;\nErro ao criar o arquivo de dataset &#39;{nome_arquivo}&#39; na pasta &#39;{caminho_com_data}&#39;. &#34;
                              f&#34;Permissão de gravação negada!\n&#34;)
                        exit(PERMISSION_ERROR)

                    for i in range(len(sentencas)):
                        arq_dataset.write(f&#34;{sentencas[i]}\n&#34;)

            if arq_dataset:
                arq_dataset.close()

            processado = True
        elif codproc == &#39;classificacao&#39;:
            arq_dataset = None

            # Não aplicável para classificação
            filtro = &#39;n/a&#39;
            nome_modelo = &#39;n/a&#39;

            cont = 1

            print(&#34;\n=&gt; Etapa: Gerando datasets de classificação.\n&#34;)

            for cod_arq, subsecoes in documentos_subsecoes.items():
                print(f&#34;   Documento ({cont}/{tam_documentos_subsecoes}): {cod_arq} &#34;, end=&#39;&#39;, flush=True)

                caminho_destino_dataset = os.path.join(caminho_com_data, cod_arq.split(&#39;.&#39;)[0] + &#39;.csv&#39;)

                try:
                    arq_dataset = open(caminho_destino_dataset, &#39;w&#39;)
                except PermissionError:
                    print(f&#34;\nErro ao criar o arquivo de dataset na pasta &#39;{caminho_com_data}&#39;. &#34;
                          f&#34;Permissão de gravação negada!\n&#34;)
                    exit(PERMISSION_ERROR)

                if cabecalho:
                    arq_dataset.write(&#34;Subseção\tTipo\n&#34;)  # Adiciona um cabeçalho para facilitar a leitura do CSV

                for sub in subsecoes:
                    arq_dataset.write(f&#34;{sub}\t&#39; &#39;\n&#34;)

                arq_dataset.close()
                print(&#34;-&gt; OK&#34;)
                cont += 1

            processado = True
        elif codproc == &#39;classificacao_label&#39;:
            nome_modelo, documentos_subsecoes_classificadas = classificar_subsecoes(documentos_subsecoes)
            del documentos_subsecoes

            filtro = &#39;n/a&#39;
            arq_dataset = None
            cont = 1

            print(&#34;\n=&gt; Etapa: Gerando datasets de classificação com os possíveis labels.\n&#34;)

            for cod_arq, subsecoes in documentos_subsecoes_classificadas.items():
                print(f&#34;   Documento ({cont}/{tam_documentos_subsecoes}): {cod_arq} &#34;, end=&#39;&#39;, flush=True)

                caminho_destino_dataset = os.path.join(caminho_com_data, cod_arq.split(&#39;.&#39;)[0] + &#39;.csv&#39;)

                try:
                    arq_dataset = open(caminho_destino_dataset, &#39;w&#39;)
                except PermissionError:
                    print(f&#34;\nErro ao criar o arquivo de dataset na pasta &#39;{caminho_com_data}&#39;. &#34;
                          f&#34;Permissão de gravação negada!\n&#34;)
                    exit(PERMISSION_ERROR)

                if cabecalho:
                    arq_dataset.write(&#34;Subseção\tTipo\n&#34;)  # Adiciona um cabeçalho para facilitar a leitura do CSV

                for sub in subsecoes:
                    arq_dataset.write(f&#34;{sub[0]}\t{sub[1]}\n&#34;)

                arq_dataset.close()
                print(&#34;-&gt; OK&#34;)
                cont += 1

            processado = True

            print(&#34;\n ATENÇÃO: Os datasets foram gerados e cada seção recebeu um label, ENTRETANTO, estes labels &#34;
                  &#34;\n          precisam ser verificados, pois foram preditos por um modelo baseado em editais de \n&#34;
                  &#34;          informática e  portanto precisam de revisão ao utilizá-lo para classificar outros \n&#34;
                  &#34;          tipos de editais.\n&#34;)

        if processado:
            print(f&#34;\n - Os datasets foram gerados na pasta &#39;{caminho_com_data}&#39;\n&#34;)

            # Gera um arquivo com os parâmetros de processamento que foram utilizados na geração dos datasets
            arq_parametros_utilizados = None
            nome_arq_parametros_utilizados = os.path.join(caminho_com_data, &#39;parametros_utilizados.txt&#39;)

            try:
                arq_parametros_utilizados = open(nome_arq_parametros_utilizados, &#39;w&#39;)
            except PermissionError:
                print(f&#34;\nErro ao criar o arquivo com os parâmetros utilizados na construção dos datasets. &#34;
                      f&#34;Permissão de gravação negada na pasta &#39;{caminho_com_data}&#39;!\nProcessamento abortado.&#34;)
                exit(PERMISSION_ERROR)

            if codproc == &#39;classificacao&#39; or codproc == &#39;classificacao_label&#39;:
                # reatribui alguns valores de parâmetros que não se aplicam à classificação para a geração do arquivo
                # de parâmetros utilizados
                tamanho_minimo_sentenca = &#39;n/a&#39;
                qtd_max_sent = &#39;n/a&#39;
                retirar_sentencas_semelhantes = &#39;n/a&#39;
                escopo_global_sentencas = &#39;n/a&#39;
                limiar = &#39;n/a&#39;

            if not retirar_sentencas_semelhantes:
                escopo_global_sentencas = &#39;n/a&#39;
                limiar = &#39;n/a&#39;

            arq_parametros_utilizados.write(f&#34;# Parâmetros utilizados para construção dos datasets:\n\n&#34;
                                            f&#34;Pasta destino: {caminho_com_data}\nCódigo de processamento: {codproc}\n&#34;
                                            f&#34;Tamanho mínimo de sentença: {tamanho_minimo_sentenca}\n&#34;
                                            f&#34;Quantidade de documentos: {tam_documentos_subsecoes}\nQuantidade máxima &#34;
                                            f&#34;de sentenças por arquivo: {qtd_max_sent}\nReprocessado: {reprocessar}\n&#34;
                                            f&#34;Organizar em pastas: {organizar_em_pastas}\n&#34;
                                            f&#34;Nome do modelo utilizado na classificação das sentenças: {nome_modelo}\n&#34;
                                            f&#34;Filtro: {filtro}\nRetirar sentenças semelhantes: &#34;
                                            f&#34;{retirar_sentencas_semelhantes}\nEscopo global de sentenças: &#34;
                                            f&#34;{escopo_global_sentencas}\nLimiar para definição de sentenças similares: &#34;
                                            f&#34;{limiar}\nDocumentos:\n{lst_documentos}&#34;)
            arq_parametros_utilizados.close()

            # Marca os documentos que foram processados
            doc_erros = marcar_documentos_processados(c_mongo_meta, doc_metadados_js, codproc, reprocessar)
            if doc_erros:
                print(f&#34;ERRO: Os documentos a seguir foram processados mas não foram atualizados:\n{doc_erros}&#34;
                      f&#34;\nVerificar!&#34;)
    else:
        print(&#34;\nNão foi possível construir o(s) dataset(s). Nenhum documento atendeu ao filtro informado!&#34;)

    c_mongo_meta.fechar_conexao()

    return documentos_sentencas, caminho_com_data</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="construtor_datasets.carregar_sentencas_base"><code class="name flex">
<span>def <span class="ident">carregar_sentencas_base</span></span>(<span>caminho, nome_arquivo)</span>
</code></dt>
<dd>
<div class="desc"><p>Carrega as sentenças base para auxiliar na verificação de documentos</p>
<pre><code>@param caminho: Caminho onde as sentenças base foram salvas
@param nome_arquivo: Nome do arquivo contendo as sentenças base
@return: Lista contendo as sentenças base que foram carregadas, ou, caso o arquivo não exista, uma lista vazia
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def carregar_sentencas_base(caminho, nome_arquivo):
    &#34;&#34;&#34;
    Carrega as sentenças base para auxiliar na verificação de documentos

        @param caminho: Caminho onde as sentenças base foram salvas
        @param nome_arquivo: Nome do arquivo contendo as sentenças base
        @return: Lista contendo as sentenças base que foram carregadas, ou, caso o arquivo não exista, uma lista vazia
    &#34;&#34;&#34;
    arq = None
    caminho_arq_sentencas_base = os.path.join(caminho, nome_arquivo)

    try:
        arq = open(caminho_arq_sentencas_base, &#39;rb&#39;)
    except FileNotFoundError:
        print(f&#34;\nErro ao carregar as sentenças base. O arquivo &#39;{caminho_arq_sentencas_base}&#39; não foi encontrado! &#34;
              f&#34;Retornando uma base vazia...\n&#34;)
        return []
    except PermissionError:
        print(f&#34;\nErro ao carregar as sentenças base do caminho &#39;{caminho_arq_sentencas_base}&#39;. Permissão de leitura &#34;
              f&#34;negada!\n&#34;)
        exit(PERMISSION_ERROR)

    sentencas_base = pickle.load(arq)
    arq.close()
    return sentencas_base</code></pre>
</details>
</dd>
<dt id="construtor_datasets.classificar_subsecoes"><code class="name flex">
<span>def <span class="ident">classificar_subsecoes</span></span>(<span>documentos_subsecoes, filtro_retorno=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Classifica as subseções em dois tipos: Jurídica (JUR) ou técnica (TEC)</p>
<pre><code>@param documentos_subsecoes: Dicionário contendo os documentos como chave e uma lista se subseções como valor
@param filtro_retorno: Filtra o retorno da função por um rótulo específico
@return: nome do modelo e (Se filtro_retorno for None, dicionário com o código do arquivo como chave e como
         valor lista de tuplas contendo a subseção e a classificação dela, caso contrário, dicionário com o
         código do arquivo como chave e como valor lista com as subseções classificadas e filtradas)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def classificar_subsecoes(documentos_subsecoes, filtro_retorno=None):
    &#34;&#34;&#34;
    Classifica as subseções em dois tipos: Jurídica (JUR) ou técnica (TEC)

        @param documentos_subsecoes: Dicionário contendo os documentos como chave e uma lista se subseções como valor
        @param filtro_retorno: Filtra o retorno da função por um rótulo específico
        @return: nome do modelo e (Se filtro_retorno for None, dicionário com o código do arquivo como chave e como
                 valor lista de tuplas contendo a subseção e a classificação dela, caso contrário, dicionário com o
                 código do arquivo como chave e como valor lista com as subseções classificadas e filtradas)
    &#34;&#34;&#34;
    documentos_subsecoes_classificadas = {}
    remover_stop_words = True

    # Escolhe o modelo de classificador para classificar as seções
    modelo = carregar_modelo(&#39;RandomForestClassifier&#39;, &#39;modelos&#39;)

    print(&#34;\n=&gt; Etapa: Classificar subseções.\n&#34;)
    total = len(documentos_subsecoes)
    cont = 1
    qtd_caracteres_limpar = 0  # Ajuda na limpeza dos códigos dos arquivos que já foram impressos na tela

    for cod_arq, subsecoes in documentos_subsecoes.items():
        print(f&#34; - Arquivo: {cont}/{total} =&gt; {cod_arq + qtd_caracteres_limpar * &#39; &#39;}&#34;, end=&#39;\r&#39;, flush=True)

        # Garante que não vai ficar lixo na tela, pois no mínimo vai limpar o tamanho do último código de arquivo
        qtd_caracteres_limpar = len(cod_arq)

        # Prepara as subseções e faz a predição dos rótulos
        subsecoes_aux = np.array(subsecoes)
        subsecoes_aux = subsecoes_aux.reshape((len(subsecoes_aux), 1))
        subsecoes_aux = tratar_strings(subsecoes_aux, remover_stop_words=remover_stop_words)
        rotulos = modelo.predict(subsecoes_aux)

        if rotulos:
            subsecoes_classificadas = []

            if filtro_retorno:
                for i in range(len(subsecoes)):
                    if rotulos[i] == filtro_retorno:
                        subsecoes_classificadas.append(subsecoes[i])
            else:
                for i in range(len(subsecoes)):
                    subsecoes_classificadas.append((subsecoes[i], rotulos[i]))

            documentos_subsecoes_classificadas[cod_arq] = subsecoes_classificadas

        cont += 1
    print(&#34;\n -&gt; Pronto!&#34;)
    return modelo.nome, documentos_subsecoes_classificadas</code></pre>
</details>
</dd>
<dt id="construtor_datasets.construir_dataset"><code class="name flex">
<span>def <span class="ident">construir_dataset</span></span>(<span>data_hora, codproc, caminho, caminho_sentencas_base, tamanho_minimo_sentenca, qtd_max_sent=5, reprocessar=False, organizar_em_pastas=False, cabecalho=False, retirar_sentencas_semelhantes=False, escopo_global_sentencas=True, limiar=0.9)</span>
</code></dt>
<dd>
<div class="desc"><p>Constrói um dataset de acordo com o tipo de processamento e grava em arquivo(s)</p>
<pre><code>@param data_hora: Data e hora para compor o nome da pasta onde os datasets serão gerados
@param codproc: Código do processamento a ser realizado (ner ou classificacao)
@param caminho: Caminho onde o(s) arquivo(s) será(ão) gerados
@param caminho_sentencas_base: Caminho para obtenção e/ou gravação das sentenças base
@param tamanho_minimo_sentenca: Sentenças com tamanho menor que este parâmetro serão descartadas
@param qtd_max_sent: Quantidade máxima de sentenças em cada arquivo. Sem efeito se o parâmetro
                     'organizar_em_pastas' for False
@param reprocessar: Indica se deve fazer o reprocessamento dos documentos, caso já tenha sido construido um
                    dataset antes
@param organizar_em_pastas: Indica se os editais serão organizados em pasta (uma pasta para cada edital)
@param cabecalho: Se deve ou não inserir cabeçalho nos arquivos de dataset para classificação
@param retirar_sentencas_semelhantes: Indica se as sentenças semelhantes devem ser retiradas no momento de
                                      geração dos datasets
@param escopo_global_sentencas: Escopo para a análise de sentenças. Se True, fará a comparação com todos os
                                documentos, caso contrário a comparação será somente no próprio documento
@param limiar: Limiar para o descarte. Caso o Índice de Jaccart fique acima do limiar a sentença é descartada
@return: Dicionário com os datasets construidos e caminho onde eles foram salvos
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def construir_dataset(data_hora, codproc, caminho, caminho_sentencas_base, tamanho_minimo_sentenca, qtd_max_sent=5,
                      reprocessar=False, organizar_em_pastas=False, cabecalho=False,
                      retirar_sentencas_semelhantes=False, escopo_global_sentencas=True, limiar=0.90):
    &#34;&#34;&#34;
    Constrói um dataset de acordo com o tipo de processamento e grava em arquivo(s)

        @param data_hora: Data e hora para compor o nome da pasta onde os datasets serão gerados
        @param codproc: Código do processamento a ser realizado (ner ou classificacao)
        @param caminho: Caminho onde o(s) arquivo(s) será(ão) gerados
        @param caminho_sentencas_base: Caminho para obtenção e/ou gravação das sentenças base
        @param tamanho_minimo_sentenca: Sentenças com tamanho menor que este parâmetro serão descartadas
        @param qtd_max_sent: Quantidade máxima de sentenças em cada arquivo. Sem efeito se o parâmetro
                             &#39;organizar_em_pastas&#39; for False
        @param reprocessar: Indica se deve fazer o reprocessamento dos documentos, caso já tenha sido construido um
                            dataset antes
        @param organizar_em_pastas: Indica se os editais serão organizados em pasta (uma pasta para cada edital)
        @param cabecalho: Se deve ou não inserir cabeçalho nos arquivos de dataset para classificação
        @param retirar_sentencas_semelhantes: Indica se as sentenças semelhantes devem ser retiradas no momento de
                                              geração dos datasets
        @param escopo_global_sentencas: Escopo para a análise de sentenças. Se True, fará a comparação com todos os
                                        documentos, caso contrário a comparação será somente no próprio documento
        @param limiar: Limiar para o descarte. Caso o Índice de Jaccart fique acima do limiar a sentença é descartada
        @return: Dicionário com os datasets construidos e caminho onde eles foram salvos
    &#34;&#34;&#34;
    documentos_sentencas = {}  # A chave é o código do arquivo e o valor é uma lista de sentenças
    caminho_com_data = &#34;&#34;  # Guarda o caminho onde os datasets serão gerados

    c_mongo_meta = ClienteGenerico(&#39;MongoDB&#39;, &#39;localhost&#39;, 27017, &#39;db_metadados&#39;)
    doc_metadados_js = obter_metadados_documentos(c_mongo_meta, codproc, reprocessar)

    if doc_metadados_js:
        print(&#34;\n                 #------- Processando documentos -------#\n&#34;)

        # Ajuda a organizar a pasta de datasets
        caminho_com_data = os.path.join(caminho, f&#34;{codproc}_{data_hora}&#34;)

        documentos_subsecoes = obter_subsecoes_documentos(doc_metadados_js)
        tam_documentos_subsecoes = len(documentos_subsecoes)
        lst_documentos = list(documentos_subsecoes.keys())
        processado = False

        # Guardam esses dados para informar no arquivo de parâmetros utilizados
        filtro = &#39;&#39;
        nome_modelo = &#39;&#39;

        # Cria uma pasta para armazenar os datasets
        try:
            os.makedirs(caminho_com_data, exist_ok=True)
        except PermissionError:
            print(
                f&#34;\nErro ao criar a pasta &#39;{caminho_com_data}&#39; para guardar os datasets. &#34;
                f&#34;Permissão de escrita negada!\n&#34;)
            exit(PERMISSION_ERROR)

        if codproc == &#39;ner&#39;:
            filtro = &#39;JUR&#39;
            nome_modelo, documentos_subsecoes_classificadas = classificar_subsecoes(documentos_subsecoes,
                                                                                    filtro_retorno=filtro)
            del documentos_subsecoes
            documentos_sentencas = dividir_em_sentencas(documentos_subsecoes_classificadas, tamanho_minimo_sentenca)
            del documentos_subsecoes_classificadas

            if retirar_sentencas_semelhantes:
                documentos_sentencas = \
                    retirar_sentencas_similares(documentos_sentencas, caminho_sentencas_base, &#34;SentencasBaseNER.pkl&#34;,
                                                caminho_com_data, reprocessar=reprocessar,
                                                escopo_global_sentencas=escopo_global_sentencas, limiar=limiar)
            arq_dataset = None

            print(&#34;\n=&gt; Etapa: Gerando arquivos.\n&#34;)

            if organizar_em_pastas:
                # Trata a quantidade máxima de sentenças para evitar problemas
                if qtd_max_sent &lt; 5:
                    qtd_max_sent = 5

                for cod_arq, sentencas in documentos_sentencas.items():
                    nome_pasta = cod_arq.split(&#39;.&#39;)[0]  # Retira a extensão do código do arquivo
                    nome_arquivo = &#39;_&#39;.join(nome_pasta.split(&#39;_&#39;)[1:])  # Retira a prefixo numérico único

                    if len(sentencas) == 0:
                        nome_pasta = &#34;DOCUMENTO_SEM_SENTENÇAS_ÚNICAS_&#34; + nome_pasta

                    # Cria uma pasta para cada documento (sem a extensão (ex: .pdf) do código do arquivo)
                    pasta_destino_dataset = os.path.join(caminho_com_data, nome_pasta)

                    try:
                        os.makedirs(pasta_destino_dataset, exist_ok=True)
                    except PermissionError:
                        print(
                            f&#34;\nErro ao criar a pasta &#39;{pasta_destino_dataset}&#39; para guardar os datasets. &#34;
                            f&#34;Permissão de escrita negada!\n&#34;)
                        exit(PERMISSION_ERROR)

                    seq = 1  # Sequência numérica para compor os nomes dos datasets

                    for i in range(len(sentencas)):
                        # Gera um arquivo diferente a cada &#39;qtd_max_sent&#39; sentenças
                        if (i % qtd_max_sent) == 0:
                            if arq_dataset:
                                arq_dataset.close()

                            caminho_destino_dataset = os.path.join(pasta_destino_dataset, f&#34;{nome_arquivo}_{seq}.txt&#34;)

                            try:
                                arq_dataset = open(caminho_destino_dataset, &#39;w&#39;)
                            except PermissionError:
                                print(f&#34;\nErro ao criar o arquivo de dataset na pasta &#39;{pasta_destino_dataset}&#39;. &#34;
                                      f&#34;Permissão de gravação negada!\n&#34;)
                                exit(PERMISSION_ERROR)

                            seq += 1

                        arq_dataset.write(f&#34;{sentencas[i]}\n&#34;)
            else:
                # Não aplicável quando os documentos não forem organizados por pasta
                qtd_max_sent = &#39;n/a&#39;

                for cod_arq, sentencas in documentos_sentencas.items():
                    nome_arquivo = cod_arq.split(&#39;.&#39;)[0] + &#39;.txt&#39;
                    caminho_destino_dataset = os.path.join(caminho_com_data, nome_arquivo)

                    try:
                        arq_dataset = open(caminho_destino_dataset, &#39;w&#39;)
                    except PermissionError:
                        print(f&#34;\nErro ao criar o arquivo de dataset &#39;{nome_arquivo}&#39; na pasta &#39;{caminho_com_data}&#39;. &#34;
                              f&#34;Permissão de gravação negada!\n&#34;)
                        exit(PERMISSION_ERROR)

                    for i in range(len(sentencas)):
                        arq_dataset.write(f&#34;{sentencas[i]}\n&#34;)

            if arq_dataset:
                arq_dataset.close()

            processado = True
        elif codproc == &#39;classificacao&#39;:
            arq_dataset = None

            # Não aplicável para classificação
            filtro = &#39;n/a&#39;
            nome_modelo = &#39;n/a&#39;

            cont = 1

            print(&#34;\n=&gt; Etapa: Gerando datasets de classificação.\n&#34;)

            for cod_arq, subsecoes in documentos_subsecoes.items():
                print(f&#34;   Documento ({cont}/{tam_documentos_subsecoes}): {cod_arq} &#34;, end=&#39;&#39;, flush=True)

                caminho_destino_dataset = os.path.join(caminho_com_data, cod_arq.split(&#39;.&#39;)[0] + &#39;.csv&#39;)

                try:
                    arq_dataset = open(caminho_destino_dataset, &#39;w&#39;)
                except PermissionError:
                    print(f&#34;\nErro ao criar o arquivo de dataset na pasta &#39;{caminho_com_data}&#39;. &#34;
                          f&#34;Permissão de gravação negada!\n&#34;)
                    exit(PERMISSION_ERROR)

                if cabecalho:
                    arq_dataset.write(&#34;Subseção\tTipo\n&#34;)  # Adiciona um cabeçalho para facilitar a leitura do CSV

                for sub in subsecoes:
                    arq_dataset.write(f&#34;{sub}\t&#39; &#39;\n&#34;)

                arq_dataset.close()
                print(&#34;-&gt; OK&#34;)
                cont += 1

            processado = True
        elif codproc == &#39;classificacao_label&#39;:
            nome_modelo, documentos_subsecoes_classificadas = classificar_subsecoes(documentos_subsecoes)
            del documentos_subsecoes

            filtro = &#39;n/a&#39;
            arq_dataset = None
            cont = 1

            print(&#34;\n=&gt; Etapa: Gerando datasets de classificação com os possíveis labels.\n&#34;)

            for cod_arq, subsecoes in documentos_subsecoes_classificadas.items():
                print(f&#34;   Documento ({cont}/{tam_documentos_subsecoes}): {cod_arq} &#34;, end=&#39;&#39;, flush=True)

                caminho_destino_dataset = os.path.join(caminho_com_data, cod_arq.split(&#39;.&#39;)[0] + &#39;.csv&#39;)

                try:
                    arq_dataset = open(caminho_destino_dataset, &#39;w&#39;)
                except PermissionError:
                    print(f&#34;\nErro ao criar o arquivo de dataset na pasta &#39;{caminho_com_data}&#39;. &#34;
                          f&#34;Permissão de gravação negada!\n&#34;)
                    exit(PERMISSION_ERROR)

                if cabecalho:
                    arq_dataset.write(&#34;Subseção\tTipo\n&#34;)  # Adiciona um cabeçalho para facilitar a leitura do CSV

                for sub in subsecoes:
                    arq_dataset.write(f&#34;{sub[0]}\t{sub[1]}\n&#34;)

                arq_dataset.close()
                print(&#34;-&gt; OK&#34;)
                cont += 1

            processado = True

            print(&#34;\n ATENÇÃO: Os datasets foram gerados e cada seção recebeu um label, ENTRETANTO, estes labels &#34;
                  &#34;\n          precisam ser verificados, pois foram preditos por um modelo baseado em editais de \n&#34;
                  &#34;          informática e  portanto precisam de revisão ao utilizá-lo para classificar outros \n&#34;
                  &#34;          tipos de editais.\n&#34;)

        if processado:
            print(f&#34;\n - Os datasets foram gerados na pasta &#39;{caminho_com_data}&#39;\n&#34;)

            # Gera um arquivo com os parâmetros de processamento que foram utilizados na geração dos datasets
            arq_parametros_utilizados = None
            nome_arq_parametros_utilizados = os.path.join(caminho_com_data, &#39;parametros_utilizados.txt&#39;)

            try:
                arq_parametros_utilizados = open(nome_arq_parametros_utilizados, &#39;w&#39;)
            except PermissionError:
                print(f&#34;\nErro ao criar o arquivo com os parâmetros utilizados na construção dos datasets. &#34;
                      f&#34;Permissão de gravação negada na pasta &#39;{caminho_com_data}&#39;!\nProcessamento abortado.&#34;)
                exit(PERMISSION_ERROR)

            if codproc == &#39;classificacao&#39; or codproc == &#39;classificacao_label&#39;:
                # reatribui alguns valores de parâmetros que não se aplicam à classificação para a geração do arquivo
                # de parâmetros utilizados
                tamanho_minimo_sentenca = &#39;n/a&#39;
                qtd_max_sent = &#39;n/a&#39;
                retirar_sentencas_semelhantes = &#39;n/a&#39;
                escopo_global_sentencas = &#39;n/a&#39;
                limiar = &#39;n/a&#39;

            if not retirar_sentencas_semelhantes:
                escopo_global_sentencas = &#39;n/a&#39;
                limiar = &#39;n/a&#39;

            arq_parametros_utilizados.write(f&#34;# Parâmetros utilizados para construção dos datasets:\n\n&#34;
                                            f&#34;Pasta destino: {caminho_com_data}\nCódigo de processamento: {codproc}\n&#34;
                                            f&#34;Tamanho mínimo de sentença: {tamanho_minimo_sentenca}\n&#34;
                                            f&#34;Quantidade de documentos: {tam_documentos_subsecoes}\nQuantidade máxima &#34;
                                            f&#34;de sentenças por arquivo: {qtd_max_sent}\nReprocessado: {reprocessar}\n&#34;
                                            f&#34;Organizar em pastas: {organizar_em_pastas}\n&#34;
                                            f&#34;Nome do modelo utilizado na classificação das sentenças: {nome_modelo}\n&#34;
                                            f&#34;Filtro: {filtro}\nRetirar sentenças semelhantes: &#34;
                                            f&#34;{retirar_sentencas_semelhantes}\nEscopo global de sentenças: &#34;
                                            f&#34;{escopo_global_sentencas}\nLimiar para definição de sentenças similares: &#34;
                                            f&#34;{limiar}\nDocumentos:\n{lst_documentos}&#34;)
            arq_parametros_utilizados.close()

            # Marca os documentos que foram processados
            doc_erros = marcar_documentos_processados(c_mongo_meta, doc_metadados_js, codproc, reprocessar)
            if doc_erros:
                print(f&#34;ERRO: Os documentos a seguir foram processados mas não foram atualizados:\n{doc_erros}&#34;
                      f&#34;\nVerificar!&#34;)
    else:
        print(&#34;\nNão foi possível construir o(s) dataset(s). Nenhum documento atendeu ao filtro informado!&#34;)

    c_mongo_meta.fechar_conexao()

    return documentos_sentencas, caminho_com_data</code></pre>
</details>
</dd>
<dt id="construtor_datasets.dividir_em_sentencas"><code class="name flex">
<span>def <span class="ident">dividir_em_sentencas</span></span>(<span>documentos_subsecoes, tamanho_minimo_sentenca)</span>
</code></dt>
<dd>
<div class="desc"><p>Divide as subseções em sentenças</p>
<pre><code>@param documentos_subsecoes: Dicionário cuja chave é o código do arquivo e o valor lista de subseções que serão
                             divididas em sentenças
@param tamanho_minimo_sentenca: Sentenças com tamanho menor que este parâmetro serão descartadas
@return: Dicionário cuja chave é o código do arquivo e o valor é uma lista de sentenças
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dividir_em_sentencas(documentos_subsecoes, tamanho_minimo_sentenca):
    &#34;&#34;&#34;
    Divide as subseções em sentenças

        @param documentos_subsecoes: Dicionário cuja chave é o código do arquivo e o valor lista de subseções que serão
                                     divididas em sentenças
        @param tamanho_minimo_sentenca: Sentenças com tamanho menor que este parâmetro serão descartadas
        @return: Dicionário cuja chave é o código do arquivo e o valor é uma lista de sentenças
    &#34;&#34;&#34;
    print(&#34;\n=&gt; Etapa: Dividir em sentenças.\n&#34;)

    nlp = None

    modelo_spacy = &#39;pt_core_news_lg&#39;
    print(f&#34; - Carregando o modelo &#39;{modelo_spacy}&#39; do Spacy... &#34;, end=&#39;&#39;, flush=True)

    try:
        nlp = spacy.load(modelo_spacy)
    except OSError:
        print(f&#34;\n\n    ERRO: Não foi possível encontrar o modelo &#39;{modelo_spacy}&#39;. Confira o nome do modelo. Caso &#34;
              f&#34;esteja correto, faça o download dele utilizando o comando &#39;python -m spacy download {modelo_spacy}&#39; num &#34;
              f&#34;terminal.\n&#34;)
        exit(SPACY_MODEL_NOT_FOUND_ERROR)

    print(&#34;-&gt; Pronto!&#34;)

    documentos_sentencas = {}
    cont = 1
    total = len(documentos_subsecoes)

    print(&#34;\n - Preparando para gerar os arquivos:\n&#34;)

    for cod_arq, subsecoes in documentos_subsecoes.items():
        sentencas = []
        print(f&#34;   Documento ({cont}/{total}): {cod_arq} &#34;, end=&#39;&#39;, flush=True)

        for sub in subsecoes:
            nlp_doc = nlp(sub)

            for sent in nlp_doc.sents:
                lst_tokens = [t.text for t in sent]

                if len(lst_tokens) &gt;= tamanho_minimo_sentenca:
                    sentencas.append(&#34; &#34;.join(lst_tokens))

        documentos_sentencas[cod_arq] = sentencas
        print(&#34;-&gt; OK&#34;)
        cont += 1

    return documentos_sentencas</code></pre>
</details>
</dd>
<dt id="construtor_datasets.marcar_documentos_processados"><code class="name flex">
<span>def <span class="ident">marcar_documentos_processados</span></span>(<span>c_mongo_meta, doc_metadados_js, codproc, reprocessado=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Marca os documentos que foram processados</p>
<pre><code>@param c_mongo_meta: Base de dados onde estão os metadados dos documentos
@param doc_metadados_js: Lista com os documentos que foram processados
@param codproc: Código de processamento
@param reprocessado: Indica se foi feito o reprocessamento dos documentos
@return: Lista com códigos de arquivos que tiveram erro na marcação. Se tudo ocorrer bem, esta lista será vazia
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def marcar_documentos_processados(c_mongo_meta, doc_metadados_js, codproc, reprocessado=False):
    &#34;&#34;&#34;
    Marca os documentos que foram processados

        @param c_mongo_meta: Base de dados onde estão os metadados dos documentos
        @param doc_metadados_js: Lista com os documentos que foram processados
        @param codproc: Código de processamento
        @param reprocessado: Indica se foi feito o reprocessamento dos documentos
        @return: Lista com códigos de arquivos que tiveram erro na marcação. Se tudo ocorrer bem, esta lista será vazia
    &#34;&#34;&#34;
    doc_erros = []  # Guarda os códigos dos documentos caso haja erro na marcação do processamento

    # Se não for reprocessamento, atualiza os códigos de processamento. Se for reprocessamento, não atualiza a lista,
    # pois o código já se encontra nela.
    if not reprocessado:
        for doc_meta in doc_metadados_js:
            doc_cod_processamento = doc_meta[&#39;Documento__cod_processamento&#39;]

            # Se o documento ainda não foi processado com algum código de processamento, retira o código &#39;a processar&#39;
            # e grava o &#39;codproc&#39;
            if &#39;a processar&#39; in doc_cod_processamento:
                doc_cod_processamento = codproc
            else:  # Se já foi processado com algum código, acrescenta o &#39;codproc&#39; na string de códigos de processamento
                doc_cod_processamento += f&#34;, {codproc}&#34;

            resultado = c_mongo_meta.alterar(&#39;col_metadados_docs&#39;, &#39;Documento__codigo_arq&#39;,
                                             doc_meta[&#39;Documento__codigo_arq&#39;], &#39;Documento__cod_processamento&#39;,
                                             doc_cod_processamento)

            if not resultado:
                doc_erros.append(doc_meta[&#39;Documento__codigo_arq&#39;])

    return doc_erros</code></pre>
</details>
</dd>
<dt id="construtor_datasets.obter_metadados_documentos"><code class="name flex">
<span>def <span class="ident">obter_metadados_documentos</span></span>(<span>c_mongo_meta, codproc, reprocessar=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Obtém os metadados dos documentos que precisam ser processados</p>
<pre><code>@param c_mongo_meta: Base de dados onde estão os metadados dos documentos
@param codproc: Código de processamento para realização do filtro na busca
@param reprocessar: Indica se deve fazer o reprocessamento dos documentos, caso já tenha sido gerado o dataset
@return: Metadados dos documentos que ainda não foram processados. Se for para reprocessar, retorna todos os
         que atendam ao filtro 'codproc'
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def obter_metadados_documentos(c_mongo_meta, codproc, reprocessar=False):
    &#34;&#34;&#34;
    Obtém os metadados dos documentos que precisam ser processados
        
        @param c_mongo_meta: Base de dados onde estão os metadados dos documentos
        @param codproc: Código de processamento para realização do filtro na busca
        @param reprocessar: Indica se deve fazer o reprocessamento dos documentos, caso já tenha sido gerado o dataset
        @return: Metadados dos documentos que ainda não foram processados. Se for para reprocessar, retorna todos os
                 que atendam ao filtro &#39;codproc&#39;
    &#34;&#34;&#34;
    # Obtém os metadados dos documentos que serão processados
    if not reprocessar:
        # Traz todos os documentos que não tem o &#39;codproc&#39;
        doc_metadados_js = c_mongo_meta.buscar_todos(&#39;col_metadados_docs&#39;, &#39;Documento__cod_processamento&#39;,
                                                     f&#34;^((?!{codproc}).)*$&#34;)
    else:
        doc_metadados_js = c_mongo_meta.buscar_todos(&#39;col_metadados_docs&#39;, &#39;Documento__cod_processamento&#39;, codproc)

    return list(doc_metadados_js)</code></pre>
</details>
</dd>
<dt id="construtor_datasets.obter_subsecoes_documentos"><code class="name flex">
<span>def <span class="ident">obter_subsecoes_documentos</span></span>(<span>doc_metadados_js)</span>
</code></dt>
<dd>
<div class="desc"><p>Obtém as subseções dos documentos que precisam ser processados para criação do dataset</p>
<pre><code>@param doc_metadados_js: Metadados dos documentos que serão processados
@return: Dicionário cuja chave é o código do arquivo e o valor é uma lista contendo as subseções do documento
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def obter_subsecoes_documentos(doc_metadados_js):
    &#34;&#34;&#34;
    Obtém as subseções dos documentos que precisam ser processados para criação do dataset

        @param doc_metadados_js: Metadados dos documentos que serão processados
        @return: Dicionário cuja chave é o código do arquivo e o valor é uma lista contendo as subseções do documento
    &#34;&#34;&#34;
    print(f&#34;\n=&gt; Etapa: Obtendo subseções...&#34;, end=&#39;&#39;, flush=True)

    c_mongo_doc = ClienteGenerico(&#39;MongoDB&#39;, &#39;localhost&#39;, 27017, &#39;db_documentos&#39;)
    documentos_subsecoes = {}  # A chave é o código do arquivo e o valor é uma lista contendo as subseções do documento

    # Obtém os documentos (através dos metadados) que serão processados
    for doc_meta in doc_metadados_js:
        doc = c_mongo_doc.buscar_um(&#39;col_editais&#39;, &#39;Edital__codigo_arq&#39;, doc_meta[&#39;Documento__codigo_arq&#39;])
        subsecoes = []

        if doc:
            doc_desserializado = ser.desserializar(doc)

            for s in doc_desserializado.secoes:
                for sb in s.subsecoes:
                    subsecoes.append(sb.descricao)

            documentos_subsecoes[doc_meta[&#39;Documento__codigo_arq&#39;]] = subsecoes

    c_mongo_doc.fechar_conexao()
    print(&#34; -&gt; Pronto!&#34;)
    return documentos_subsecoes</code></pre>
</details>
</dd>
<dt id="construtor_datasets.retirar_sentencas_similares"><code class="name flex">
<span>def <span class="ident">retirar_sentencas_similares</span></span>(<span>documentos_sentencas, caminho_sentencas_base, nome_arq_sents_base, caminho_relatorio, reprocessar=False, escopo_global_sentencas=True, limiar=0.9)</span>
</code></dt>
<dd>
<div class="desc"><p>Retira sentenças similares de documentos, com base no Índice de Jaccart e um limiar.</p>
<pre><code>@param documentos_sentencas: Dicionário onde a chave é o código do arquivo e o valor é uma lista de sentenças
@param caminho_sentencas_base: Caminho para obtenção e/ou gravação das sentenças base
@param nome_arq_sents_base: Nome do arquivo de sentenças base
@param caminho_relatorio: Caminho onde será gerado o relatório com os detalhes das sentenças descartadas
@param reprocessar: Indica se deve fazer o reprocessamento dos documentos, caso já tenha sido construido um
                    dataset antes
@param escopo_global_sentencas: Escopo para a análise de sentenças. Se True, fará a comparação com todos os
                                documentos, caso contrário a comparação será somente no próprio documento
@param limiar: Limiar para o descarte. Caso o Índice de Jaccart fique acima do limiar a sentença é descartada
@return: documentos_sentencas sem as sentenças similares, se houverem
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def retirar_sentencas_similares(documentos_sentencas, caminho_sentencas_base, nome_arq_sents_base, caminho_relatorio,
                                reprocessar=False, escopo_global_sentencas=True, limiar=0.90):
    &#34;&#34;&#34;
    Retira sentenças similares de documentos, com base no Índice de Jaccart e um limiar.

        @param documentos_sentencas: Dicionário onde a chave é o código do arquivo e o valor é uma lista de sentenças
        @param caminho_sentencas_base: Caminho para obtenção e/ou gravação das sentenças base
        @param nome_arq_sents_base: Nome do arquivo de sentenças base
        @param caminho_relatorio: Caminho onde será gerado o relatório com os detalhes das sentenças descartadas
        @param reprocessar: Indica se deve fazer o reprocessamento dos documentos, caso já tenha sido construido um
                            dataset antes
        @param escopo_global_sentencas: Escopo para a análise de sentenças. Se True, fará a comparação com todos os
                                        documentos, caso contrário a comparação será somente no próprio documento
        @param limiar: Limiar para o descarte. Caso o Índice de Jaccart fique acima do limiar a sentença é descartada
        @return: documentos_sentencas sem as sentenças similares, se houverem
    &#34;&#34;&#34;
    print(f&#34;\n=&gt; Etapa: Retirando sentenças similares (escopo_global_sentencas={escopo_global_sentencas}; &#34;
          f&#34;limiar={limiar}).\n&#34;)

    # Guarda informações das sentenças de todos os documentos para avaliação da similaridade.
    # Estrutura de cada sentença base da lista: (&#34;nome do arquivo&#34;, sentença, set da sentença tratada). Exemplo:
    # (&#34;Edital_001.pdf&#34;, &#34;O índice de reajuste não foi o maior de 2021&#34;,
    # {&#39;2021&#39;, &#39;de&#39;, &#39;foi&#39;, &#39;maior&#39;, &#39;nao&#39;, &#39;o&#39;, &#39;reajuste&#39;, &#39;indice&#39;})
    # Obs.: Não carrega o arquivo de sentenças base se for reprocessar, pois se carregar, todas as sentenças serão
    # descartadas!
    if escopo_global_sentencas and not reprocessar:
        sentencas_base = carregar_sentencas_base(caminho_sentencas_base, nome_arq_sents_base)
    else:
        sentencas_base = []

    cont = 1
    qtd_documentos = len(documentos_sentencas)
    sentencas_descartadas_por_arquivo = {}
    total_global_sentencas = 0
    total_global_sentencas_descartadas = 0

    for cod_arq, sentencas in documentos_sentencas.items():
        if not escopo_global_sentencas:
            sentencas_base = []

        print(f&#34;   Documento ({cont}/{qtd_documentos}): {cod_arq} &#34;, end=&#39;&#39;, flush=True)

        sentencas_a_inserir = []
        sentencas_descartadas = []

        for sent_aval in sentencas:
            tipo_sent_aval = type(sent_aval)

            if tipo_sent_aval is not str and tipo_sent_aval is not tuple:
                print(f&#34;\n\nO Formato &#39;{tipo_sent_aval}&#39; não é aceito para as sentenças. Sentença errada: &#39;{sent_aval}&#39;&#34;
                      f&#34;. Formatos aceitos: &#39;str&#39; e &#39;tuple&#39;.\n&#34;)
                exit(INVALID_CONTENT)

            sent_inserir = sent_aval

            # Trata o caso das sentenças no formato JSON vindas da rotina para conversão dos arquivos anotados no
            # Doccano para o formato CONLL. Neste caso, sent_aval será uma tupla contendo na posição 0 o número da linha
            # da sentença no arquivo de sentenças anotadas e na posição 1 a sentença no formato JSON
            if tipo_sent_aval is tuple:
                sent_aval = sent_aval[1][&#39;data&#39;]

            if len(sent_aval) == 0:
                continue

            # Retira acentos, cedilhas, etc... (transforma em codificação ASCII) para considerar palavras acentuadas
            # corretamente iguais a outras que não foram
            sent_aval_aux = normalize(&#39;NFKD&#39;, sent_aval).encode(&#39;ASCII&#39;, &#39;ignore&#39;).decode(&#39;ASCII&#39;)

            sent_aval_aux = set(sent_aval_aux.lower().split())
            inserir = True

            # Estrutura da sent_base do for abaixo: (&#34;nome do arquivo&#34;, sentença, set da sentença tratada). Exemplo:
            # (&#34;Edital_001.pdf&#34;, &#34;O índice de reajuste não foi o maior de 2021&#34;,
            # {&#39;2021&#39;, &#39;de&#39;, &#39;foi&#39;, &#39;maior&#39;, &#39;nao&#39;, &#39;o&#39;, &#39;reajuste&#39;, &#39;indice&#39;})

            sentenca_semelhante = None
            ind_jaccart_sentenca_descartada = None

            for sent_base in sentencas_base:
                ind_jaccart = len(sent_base[2].intersection(sent_aval_aux)) / len(sent_base[2].union(sent_aval_aux))

                if ind_jaccart &gt; limiar:
                    inserir = False
                    sentenca_semelhante = sent_base
                    ind_jaccart_sentenca_descartada = ind_jaccart
                    break

            if inserir:
                sentencas_base.append((cod_arq, sent_aval, sent_aval_aux))
                sentencas_a_inserir.append(sent_inserir)
            else:
                sentencas_descartadas.append((sent_aval, sent_aval_aux, ind_jaccart_sentenca_descartada,
                                              sentenca_semelhante))

        total_sentencas = len(sentencas)
        qtd_sentencas_descartadas = len(sentencas_descartadas)
        total_global_sentencas += total_sentencas
        total_global_sentencas_descartadas += qtd_sentencas_descartadas

        print(f&#34;(Sentenças descartadas/total: {qtd_sentencas_descartadas}/{total_sentencas} - &#34;
              f&#34;{(qtd_sentencas_descartadas / total_sentencas) * 100:.2f}%)-&gt; OK&#34;)

        documentos_sentencas[cod_arq] = sentencas_a_inserir
        sentencas_descartadas_por_arquivo[cod_arq] = (sentencas_descartadas, total_sentencas)

        cont += 1

    total_sentencas_aproveitadas = total_global_sentencas - total_global_sentencas_descartadas

    if total_global_sentencas &gt; 0:
        print(f&#34;\n# Resultado: Das {total_global_sentencas} sentenças encontradas nos {qtd_documentos} documentos, &#34;
              f&#34;foram aproveitadas {total_sentencas_aproveitadas} &#34;
              f&#34;({(total_sentencas_aproveitadas / total_global_sentencas) * 100:.2f}%) e descartadas &#34;
              f&#34;{total_global_sentencas_descartadas} &#34;
              f&#34;({(total_global_sentencas_descartadas / total_global_sentencas) * 100:.2f}%)\n&#34;)

        # Gera um arquivo com o detalhamento das sentenças descartadas
        caminho_arq_detalhes_descarte = os.path.join(caminho_relatorio, &#34;detalhamento_sentencas_descartadas.txt&#34;)

        with open(caminho_arq_detalhes_descarte, &#34;w&#34;) as arq:
            for cod_arq, sent_descartadas in sentencas_descartadas_por_arquivo.items():
                qtd_sentencas_descartadas = len(sent_descartadas[0])
                qtd_total_sentencas = sent_descartadas[1]

                arq.write(f&#34;# Documento: {cod_arq} - Qtd. sentenças descartadas/Qtd. sentenças: &#34;
                          f&#34;{qtd_sentencas_descartadas}/{qtd_total_sentencas} &#34;
                          f&#34;({(qtd_sentencas_descartadas / qtd_total_sentencas) * 100:.2f}%)\n\n&#34;)

                for sent_descartada in sent_descartadas[0]:
                    arq.write(f&#34;   Sentença descartada...............: {sent_descartada[0]}\n&#34;)
                    arq.write(f&#34;   Set sent. descartada..............: {sent_descartada[1]}\n&#34;)
                    arq.write(f&#34;   Índice de Jaccart.................: {sent_descartada[2]}\n&#34;)
                    arq.write(&#34;   &gt; Dados da sentença base semelhante\n&#34;)
                    arq.write(f&#34;                - Nome do documento..: {sent_descartada[3][0]}\n&#34;)
                    arq.write(f&#34;                - Sentença base......: {sent_descartada[3][1]}\n&#34;)
                    arq.write(f&#34;                - Set sent. base.....: {sent_descartada[3][2]}\n\n&#34;)

                arq.write(&#34;\n\n&#34;)

    if escopo_global_sentencas:
        salvar_sentencas_base(caminho_sentencas_base, nome_arq_sents_base, sentencas_base)

    return documentos_sentencas</code></pre>
</details>
</dd>
<dt id="construtor_datasets.salvar_sentencas_base"><code class="name flex">
<span>def <span class="ident">salvar_sentencas_base</span></span>(<span>caminho, nome_arquivo, sentencas_base)</span>
</code></dt>
<dd>
<div class="desc"><p>Salva as sentenças base para auxiliar na verificação de documentos futuros</p>
<pre><code>@param caminho: Caminho onde as sentenças base serão salvas
@param nome_arquivo: Nome do arquivo onde as sentenças base serão salvas
@param sentencas_base: Lista contendo as sentenças base que serão salvas
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def salvar_sentencas_base(caminho, nome_arquivo, sentencas_base):
    &#34;&#34;&#34;
    Salva as sentenças base para auxiliar na verificação de documentos futuros

        @param caminho: Caminho onde as sentenças base serão salvas
        @param nome_arquivo: Nome do arquivo onde as sentenças base serão salvas
        @param sentencas_base: Lista contendo as sentenças base que serão salvas
    &#34;&#34;&#34;
    arq = None
    caminho_arq_sentencas_base = os.path.join(caminho, nome_arquivo)

    try:
        arq = open(caminho_arq_sentencas_base, &#39;wb&#39;)
    except FileNotFoundError:
        print(f&#34;\nErro ao salvar as sentenças base. O caminho &#39;{caminho}&#39; não existe!\n&#34;)
        exit(FILE_NOT_FOUND_ERROR)
    except PermissionError:
        print(f&#34;\nErro ao salvar as sentenças base no caminho &#39;{caminho}&#39;. Permissão de escrita negada!\n&#34;)
        exit(PERMISSION_ERROR)

    pickle.dump(sentencas_base, arq)
    arq.close()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="construtor_datasets.carregar_sentencas_base" href="#construtor_datasets.carregar_sentencas_base">carregar_sentencas_base</a></code></li>
<li><code><a title="construtor_datasets.classificar_subsecoes" href="#construtor_datasets.classificar_subsecoes">classificar_subsecoes</a></code></li>
<li><code><a title="construtor_datasets.construir_dataset" href="#construtor_datasets.construir_dataset">construir_dataset</a></code></li>
<li><code><a title="construtor_datasets.dividir_em_sentencas" href="#construtor_datasets.dividir_em_sentencas">dividir_em_sentencas</a></code></li>
<li><code><a title="construtor_datasets.marcar_documentos_processados" href="#construtor_datasets.marcar_documentos_processados">marcar_documentos_processados</a></code></li>
<li><code><a title="construtor_datasets.obter_metadados_documentos" href="#construtor_datasets.obter_metadados_documentos">obter_metadados_documentos</a></code></li>
<li><code><a title="construtor_datasets.obter_subsecoes_documentos" href="#construtor_datasets.obter_subsecoes_documentos">obter_subsecoes_documentos</a></code></li>
<li><code><a title="construtor_datasets.retirar_sentencas_similares" href="#construtor_datasets.retirar_sentencas_similares">retirar_sentencas_similares</a></code></li>
<li><code><a title="construtor_datasets.salvar_sentencas_base" href="#construtor_datasets.salvar_sentencas_base">salvar_sentencas_base</a></code></li>
</ul>
</li>
</ul>
<p><a href="index.html" title="Voltar"><h3><< Voltar</h3></a></p>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
