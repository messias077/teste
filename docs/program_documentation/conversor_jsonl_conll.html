<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>conversor_jsonl_conll API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>conversor_jsonl_conll</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># ----------------------------------------------------------------
# Funções para conversão de arquivos anotados no Doccano e
# exportados no formato JSONL para o formato CONLL
# ----------------------------------------------------------------

import json
import os
import magic
from sklearn.model_selection import train_test_split
from src.ambiente.parametros_globais import PERMISSION_ERROR, REN_CAMINHO_ARQ_CONF, FILE_NOT_FOUND_ERROR, \
    INVALID_CONTENT, VALUE_ERROR
from src.ambiente.preparar_ambiente import inicializar_parametros, validar_pastas
from src.modulos.ren.construtor_datasets import retirar_sentencas_similares


def fatiar_sentenca(sentenca, marcadores_entidades):
    &#34;&#34;&#34;
    Fatia uma sentença com base nos marcadores dos labels e separas as entidades que receberam label, dos outros tokens

        @param sentenca: Sentença que será fatiada
        @param marcadores_entidades: Lista contendo listas com os marcadores de entidades
        @return: Lista contendo tuplas de fatias da sentença com suas respectivas tags
    &#34;&#34;&#34;
    # Prepara um dicionário com os marcadores para auxiliar no fatiamento. Ficará com o formato conforme exemplo:
    # - Marcador original: [[0, 6, &#39;Organizacao&#39;], [69, 79, &#39;Local&#39;]]
    # - Marcador auxiliar: {&#39;0-6&#39;: &#39;Organizacao&#39;, &#39;69-79&#39;: &#39;Local&#39;}
    dict_marcadores_entidades = {str(m[0]) + &#39;-&#39; + str(m[1]): m[2] for m in marcadores_entidades}

    # Cria uma lista com os índices dos marcadores para auxiliar no fatiamento. Ficará com o formato conforme exemplo:
    # - Marcador original: [[0, 6, &#39;Organizacao&#39;], [69, 79, &#39;Local&#39;]]
    # - lista auxiliar com os índices: [0, 0, 6, 69, 79, None]. Obs.: Inclusão do 0 e do None para ajudar no fatiamento
    lista_aux = [0]

    for m in marcadores_entidades:
        lista_aux.append(m[0])
        lista_aux.append(m[1])

    # Ordena a lista para evitar erros no fatiamento, pois o Doccano pode exportar a lista de marcadores fora de ordem.
    # Exemplo real de export da lista de marcadores fora de ordem:
    # {&#39;id&#39;: 78, &#39;data&#39;: &#39;14h em 12/09/17 Página 2&#39;, &#39;label&#39;: [[7, 15, &#39;Tempo&#39;], [0, 3, &#39;Tempo&#39;]]}
    # Se utilizar a lista auxiliar com os marcadores fora de ordem, o fatiamento ficará errado!
    lista_aux.sort()

    lista_aux.append(None)

    # Fatia as sentenças e atribui os labels a cada token
    sentenca_fatiada = []

    # Percorre a lista axiliar e pega os marcadores de 2 em 2 (i e i+1) e utiliza como início (i) e fim (i+1) para
    # fatiar sentença e obter as entidades (obs.: Ainda não serão tokenizadas) e atribuir as tags correspondentes.
    # A tag &#39;O&#39; não é informada pelo Doccano, ela é atribuida aqui nesta função para as entidades que não foram nomeadas
    for i in range(len(lista_aux) - 1):
        inicio = lista_aux[i]
        fim = lista_aux[i + 1]
        entidade = sentenca[inicio:fim]
        chave = str(inicio) + &#39;-&#39; + str(fim)
        tag = dict_marcadores_entidades[chave].upper() if chave in dict_marcadores_entidades else &#39;O&#39;

        # Evita fatias vazias:
        # - no caso de inicio e fim iguais (0,0): Se fatiar assim vai retornar uma string vazia;
        # - caso a sentença termine com uma entidade diferente de &#39;O&#39;, exemplo: &#34;hoje é dia 20/01/2022&#34;, nesta sentença
        #   a última entidade é uma data e o marcador de fim estará apontado para 21 (tamanho da sentença), logo, fatiar
        #   a partir da posição 21 retornará uma string vazia.
        if entidade != &#39;&#39;:
            sentenca_fatiada.append((entidade, tag))

    return sentenca_fatiada


def distribuir_tags(sentenca_fatiada):
    &#34;&#34;&#34;
    Distribui as tags entre os tokens da sentença fatiada e, caso for necessário, aninha as tags de entidades que
    possuem mais de um token

        @param sentenca_fatiada: Lista contendo tuplas de fatias da sentença com suas respectivas tags
        @return: Lista contendo tuplas de entidades com suas respectivas tags
    &#34;&#34;&#34;
    entidades = []

    for fatia in sentenca_fatiada:
        # Obs.: fatia[0] = fatia da sentença; fatia[1] = tag referente à fatia
        entidade_tokens = fatia[0].split()

        if entidade_tokens:
            tag = &#39;B-&#39; + fatia[1] if fatia[1] != &#39;O&#39; else &#39;O&#39;
            entidades.append((entidade_tokens[0], tag))

            if len(entidade_tokens) &gt; 1:
                tag = &#39;I-&#39; + fatia[1] if fatia[1] != &#39;O&#39; else &#39;O&#39;

                for e in entidade_tokens[1:]:
                    entidades.append((e, tag))

    return entidades


def carregar_arquivo(caminho):
    &#34;&#34;&#34;
    Carrega as linhas de um arquivo no formato JSONL (Arquivo com sentenças anotadas e exportadas através do Doccano)

        @param caminho: Caminho do aquivo que será carregado
        @return: Linhas lidas e codificação do arquivo
    &#34;&#34;&#34;
    # Verifica qual a codificação do arquivo
    try:
        codificacao = magic.Magic(mime_encoding=True).from_file(caminho)
    except PermissionError:
        codificacao = &#39;utf-8&#39;  # Assume que a codificação será &#39;utf-8&#39;

    arq_json = None

    try:
        arq_json = open(caminho, &#39;r&#39;, encoding=codificacao)
    except PermissionError:
        print(f&#34;Erro ao ler o arquivo &#39;{caminho}&#39;. Permissão de leitura negada!&#34;)
        return [], None
    except FileNotFoundError:
        print(f&#34;\nErro ao ler o arquivo &#39;{caminho}&#39;. Arquivo não encontrado!\n&#34;)
        exit(FILE_NOT_FOUND_ERROR)
    except LookupError:
        print(f&#34;Falhou: A Codificação &#39;{codificacao}&#39; não é suportada!&#34;)
        return [], None

    linhas = []

    try:
        linhas = arq_json.readlines()
    except UnicodeDecodeError as e:
        print(f&#34;Falha ao ler as linhas do arquivo. Erro: {e}&#34;)

    if arq_json:
        arq_json.close()

    return linhas, codificacao


def concatenar_ordenar_linhas(arq_1, arq_2):
    &#34;&#34;&#34;
    Concatena e ordena as linhas de dois arquivos JSONL

        @param arq_1: linhas lidas do primeiro arquivo
        @param arq_2: linhas lidas do segundo arquivo
        @return: Lista com as linhas dos dois arquivos concatenadas e ordenadas
    &#34;&#34;&#34;
    linhas_juntas = []

    # Obtém os IDs de cada linha do arquivo e cria tuplas auxiliares para facilitar a ordenação das linhas pelo ID
    for arq in [arq_1, arq_2]:
        for linha in arq:
            linhas_juntas.append((int(linha.split(&#34;,&#34;)[0].split(&#34;:&#34;)[1]), linha))

    linhas_juntas.sort()

    # Retira os IDs aulixiares e obtém as linhas ordenadas
    linhas_juntas_ordenadas = [linha[1] for linha in linhas_juntas]

    return linhas_juntas_ordenadas


def concatenar_arquivos_jsonl(caminho):
    &#34;&#34;&#34;
    Concatena os arquivos JSON &#39;admin.jsonl&#39; e &#39;unknown.jsonl&#39; ordenando as linhas

        @param caminho: Caminho dos aquivos que serão verificados e concatenados se necessário
    &#34;&#34;&#34;
    print(&#34;\n=&gt; Concatenando e ordenando as linhas dos arquivos JSONL:\n&#34;)

    # obtém os caminhos das pastas recursivamente e os ordena
    caminhos_pastas = [pasta for pasta, subpastas, arquivos in os.walk(caminho)]
    caminhos_pastas.sort()

    arquivos_concatenar = []  # Guarda os arquivos que serão verificados e concatenados se necessário

    # Apura quais arquivos serão concatenados
    for pasta in caminhos_pastas:
        arq_admin = os.path.join(pasta, &#34;admin.jsonl&#34;)
        arq_unknown = os.path.join(pasta, &#34;unknown.jsonl&#34;)

        # Trata o caso de existir somente o arquivo &#34;admin.jsonl&#34; ou &#34;unknown.jsonl&#34;
        if os.path.exists(arq_admin) and os.path.exists(arq_unknown):
            arquivos_concatenar.append((pasta, arq_admin, arq_unknown))
        elif os.path.exists(arq_admin) and not os.path.exists(arq_unknown):
            arquivos_concatenar.append((pasta, arq_admin))
        elif not os.path.exists(arq_admin) and os.path.exists(arq_unknown):
            arquivos_concatenar.append((pasta, arq_unknown))

    # Concatena os arquivos
    for a in arquivos_concatenar:
        codificacao = None
        arq_admin_lido = None
        arq_unknown_lido = None
        arq_sozinho_lido = None  # Guardará ou o arquivo &#39;admin.jsonl&#39; ou &#39;unknown.jsonl&#39;, caso tenha somente um deles

        if len(a) == 3:
            print(f&#34;Concatenando e ordenando os arquivos: &#39;{a[1]}&#39; e &#39;{a[2]}&#39;...&#34;, end=&#39;&#39;, flush=True)
            arq_admin_lido, codificacao = carregar_arquivo(a[1])  # Assume que a codificação dos dois arquivos é a mesma
            arq_unknown_lido, _ = carregar_arquivo(a[2])

            if not arq_admin_lido or not arq_unknown_lido:
                print(&#34;Erro no carregamento das linhas. Verifique o conteúdo dos arquivos!&#34;)
                continue
        elif len(a) == 2:
            print(f&#34;Somente carregando o arquivo &#39;{a[1]}&#39; pois só existe ele...&#34;, end=&#39;&#39;, flush=True)
            arq_sozinho_lido, codificacao = carregar_arquivo(a[1])

            if not arq_sozinho_lido:
                print(f&#34;Erro no carregamento das linhas. Verifique o conteúdo do arquivo!&#34;)
                continue

        linhas_ordenadas = []

        if arq_admin_lido:
            linhas_ordenadas = concatenar_ordenar_linhas(arq_admin_lido, arq_unknown_lido)
        elif arq_sozinho_lido:
            linhas_ordenadas = arq_sozinho_lido

        if linhas_ordenadas:
            caminho_arq_concatenado = os.path.join(a[0], &#34;concatenado.jsonl&#34;)
            arq_json_concatenado = None

            try:
                arq_json_concatenado = open(caminho_arq_concatenado, &#39;w&#39;, encoding=codificacao)
            except PermissionError:
                print(f&#34;Erro ao criar o arquivo &#39;{caminho_arq_concatenado}&#39;. Permissão de escrita negada!\n&#34;)
                exit(PERMISSION_ERROR)
            except LookupError:
                print(f&#34;Falhou: A Codificação &#39;{codificacao}&#39; não é suportada!&#34;)
                continue

            arq_json_concatenado.writelines(linhas_ordenadas)
            arq_json_concatenado.close()
            print(&#34; OK!&#34;)


def carregar_jsonl(caminho):
    &#34;&#34;&#34;
    Carrega os dados de um arquivo no formato JSONL (Arquivo com sentenças anotadas e exportadas através do Doccano)

        @param caminho: Caminho do aquivo que será carregado
        @return: Lista com tuplas com número da linha da sentença e dicionário contendo a sentença, marcação das
        entidades e tags, de cada uma das sentenças do arquivo
    &#34;&#34;&#34;
    # Verifica qual a codificação do arquivo
    try:
        codificacao = magic.Magic(mime_encoding=True).from_file(caminho)
    except PermissionError:
        codificacao = &#39;utf-8&#39;  # Assume que a codificação será &#39;utf-8&#39;

    print(f&#34;Carregando o arquivo &#39;{caminho}&#39; (Codificação=&#39;{codificacao}&#39;)&#34;, end=&#39;&#39;)

    arq_json = None

    try:
        arq_json = open(caminho, &#39;r&#39;, encoding=codificacao)
    except PermissionError:
        print(f&#34;(Erro ao ler o arquivo &#39;{caminho}&#39;. Permissão de leitura negada!&#34;, end=&#39;&#39;)
        return []
    except FileNotFoundError:
        print(f&#34;\nErro ao ler o arquivo &#39;{caminho}&#39;. Arquivo não encontrado!\n&#34;)
        exit(FILE_NOT_FOUND_ERROR)
    except LookupError:
        print(f&#34;(Falhou: A Codificação &#39;{codificacao}&#39; não é suportada!&#34;, end=&#39;&#39;)
        return []

    sentencas = []
    linhas = []

    try:
        linhas = arq_json.readlines()
    except UnicodeDecodeError as e:
        print(f&#34;Falhou: {e}&#34;, end=&#39;&#39;)

    for i in range(len(linhas)):
        try:
            sentencas.append((i+1, json.loads(linhas[i].strip(&#39;\n&#39;))))  # Guarda também o número da linha
        except json.decoder.JSONDecodeError as e:
            print(f&#34;Falhou. JSONL mal formado: {e}&#34;, end=&#39;&#39;)

    if arq_json:
        arq_json.close()

    print(&#34; -&gt; OK&#34;)

    return sentencas


def gerar_arquivo_conll(sentencas_com_tags, pasta_destino_dataset, tamanho_dataset_teste=0.0):
    &#34;&#34;&#34;
    Gera um arquivo (ou mais, caso seja especificado o tamanho do dataset de teste) no formato CONLL

        @param sentencas_com_tags: Lista contendo listas de tuplas com tokens e suas respectivas tags. Cada uma dessas
                                   listas de tuplas representa uma sentença.
        @param pasta_destino_dataset: Pasta onde será gerado o arquivo CONLL
        @param tamanho_dataset_teste: Tamanho do dataset de teste. Obs.: O dataset de teste será subdivido em teste e
                                      validação: 50% teste e 50% validação
    &#34;&#34;&#34;
    print(&#34;\n=&gt; Gerando arquivo(s) no formato CONLL... &#34;, end=&#39;&#39;)

    datasets = {}

    if tamanho_dataset_teste == 0.0:
        datasets = {&#34;dataset_ner.conll&#34;: sentencas_com_tags}
    elif tamanho_dataset_teste &gt; 0.0:
        # Divide e prepara os datasets para a geração dos arquivos CONLL
        treino = None
        teste = None
        validacao = None

        try:
            treino, teste = \
                train_test_split(sentencas_com_tags, test_size=tamanho_dataset_teste, shuffle=False, random_state=44)
            teste, validacao = train_test_split(teste, test_size=0.5, shuffle=False, random_state=44)
        except ValueError as e:
            print(f&#34;-&gt; ERRO: Arquivos CONLL não gerados. Não foi possível dividir o dataset. Mensagem do &#34;
                  f&#34;&#39;train_test_split&#39;: {e}&#34;)
            exit(VALUE_ERROR)

        datasets = {&#34;dataset_ner_train.conll&#34;: treino, &#34;dataset_ner_test.conll&#34;: teste,
                    &#34;dataset_ner_val.conll&#34;: validacao}

        print(f&#34;\n\nATENÇÃO: O dataset foi dividido (tamanho_dataset_teste={tamanho_dataset_teste}) e as partes ficaram&#34;
              f&#34; com as seguintes quantidades de sentenças:\n\n - Treino = {len(treino)}\n - Teste = {len(teste)}&#34;
              f&#34;\n - Validação = {len(validacao)}\n&#34;)
    else:
        print(f&#34;-&gt; ERRO: Arquivos CONLL não gerados. O tamanho &#39;{tamanho_dataset_teste}&#39; não é válido para o dataset &#34;
              f&#34;de teste!\n&#34;)
        exit(INVALID_CONTENT)

    for nome_arq, sentencas_com_tags in datasets.items():
        caminho_destino_dataset = os.path.join(pasta_destino_dataset, nome_arq)
        arq_dataset = None

        try:
            arq_dataset = open(caminho_destino_dataset, &#39;w&#39;)
        except PermissionError:
            print(f&#34;\nErro ao criar o arquivo de dataset na pasta &#39;{pasta_destino_dataset}&#39;. Permissão de gravação &#34;
                  f&#34;negada!\n&#34;)
            exit(PERMISSION_ERROR)

        for sentenca in sentencas_com_tags:
            for token in sentenca:
                arq_dataset.write(f&#34;{token[0]} {token[1]}\n&#34;)

            arq_dataset.write(&#34;\n&#34;)

        if arq_dataset:
            arq_dataset.close()

    print(&#34;-&gt; OK&#34;)


def gerar_estatisticas(qtd_arquivos_processados, sentencas_com_tags):
    &#34;&#34;&#34;
    Gera as estatísticas do processo de conversão dos arquivos anotados para o formato CONLL

        @param qtd_arquivos_processados: Quantidade de arquivos processados
        @param sentencas_com_tags: Lista contendo listas de tuplas com tokens e suas respectivas tags. Cada uma dessas
                                   listas de tuplas representa uma sentença.
    &#34;&#34;&#34;
    qtd_sentencas = len(sentencas_com_tags)
    total_tokens = 0
    qtd_entidades = 0

    for sentenca in sentencas_com_tags:
        total_tokens += len(sentenca)

        # Contabiliza só as entidades que tem tags começadas com &#39;B-&#39;
        for token in sentenca:
            if &#34;B-&#34; in token[1]:
                qtd_entidades += 1

    # Para evitar a divisão por zero
    if total_tokens == 0:
        print(&#34;\nNão foi possível gerar as estatísticas porque não há tokens!&#34;)
        return

    print(&#34;\n\n =&gt; Estatísticas do processo de conversão dos arquivos anotados\n&#34;)
    print(&#34;\tQuantidade de arquivos processados..................:&#34;, qtd_arquivos_processados)
    print(&#34;\tQuantidade de sentenças.............................:&#34;, qtd_sentencas)
    print(&#34;\tTotal de tokens.....................................:&#34;, total_tokens)
    print(&#34;\tQuantidade de entidades (exceto tag &#39;O&#39;)............:&#34;, qtd_entidades)
    print(f&#34;\tMédia de sentenças por arquivo......................: {(qtd_sentencas/qtd_arquivos_processados):.2f}&#34;)
    print(f&#34;\tMédia de tokens por arquivo.........................: {(total_tokens/qtd_arquivos_processados):.2f}&#34;)
    print(f&#34;\tMédia de tokens por sentença........................: {(total_tokens/qtd_sentencas):.2f}&#34;)
    print(f&#34;\tMédia de entidades (exceto tag &#39;O&#39;) por arquivo.....: {(qtd_entidades/qtd_arquivos_processados):.2f}&#34;)
    print(f&#34;\tMédia de entidades (exceto tag &#39;O&#39;) por sentença....: {(qtd_entidades/qtd_sentencas):.2f}&#34;)
    print(f&#34;\t% de tokens que são entidades (exceto tag &#39;O&#39;)......: {((qtd_entidades/total_tokens) * 100):.2f}%&#34;)


def contabilizar_entidades(caminho, entidades_por_arquivo):
    &#34;&#34;&#34;
    Contabiliza as entidades constantes nos arquivos. Mostra sumário na tela, com o total de tokens por entidade,
    grava um arquivo com o total de cada tipo de tag encontrada nos arquivos e grava um outro arquivo com os exemplos
    de entidades organizados por tag

        @param caminho: Caminho onde o arquivo com os detalhes será gravado
        @param entidades_por_arquivo: Dicionário cuja chave é o nome do arquivo e o conteúdo é outro dicionário cuja
                                      chave é a linha+id e o conteúdo uma lista de tuplas de tokens e suas respectivas
                                      tags
    &#34;&#34;&#34;
    tokens_por_arquivo = {}  # Guarda os tokens diferentes de &#39;O&#39; encontrados em cada arquivo. Organiza por arquivo.
    total_tags = {}  # Guarda o total de cada tipo de tag encontrada nos arquivos. Organiza por nome da tag.
    exemplos_entidades_por_tag = {}  # Guarda os exemplos de entidades por tag

    for a, entidades_por_linha in entidades_por_arquivo.items():
        tokens_por_arquivo[a] = []
        tokens_por_tag_linha = {}

        for linha_id, entidades in entidades_por_linha.items():
            for e in entidades:
                inicio_tag = e[1][:2]
                chave = e[1][2:]  # Pega o nome da tag sem o B-

                if inicio_tag == &#39;B-&#39; or inicio_tag == &#39;I-&#39;:
                    # Guarda os tokens marcados para o arquivo e a referida linha onde eles se encontram
                    if chave in tokens_por_tag_linha:
                        tokens_por_tag_linha[chave].append((linha_id, e))
                    else:
                        tokens_por_tag_linha[chave] = [(linha_id, e)]

                    # Guarda os exemplos por tag
                    if chave in exemplos_entidades_por_tag:
                        exemplos_entidades_por_tag[chave].append((e[0], inicio_tag))
                    else:
                        exemplos_entidades_por_tag[chave] = [(e[0], inicio_tag)]

                # Contabiliza o total de cada tipo de tag
                if inicio_tag == &#39;B-&#39;:
                    if chave in total_tags:
                        total_tags[chave] += 1
                    else:
                        total_tags[chave] = 1

        tokens_por_arquivo[a].append(tokens_por_tag_linha)

    # Gera o arquivo com os dados de tokens por arquivo
    arq = None
    caminho_arq = os.path.join(caminho, &#34;tokens_anotados_por_arquivo.txt&#34;)

    try:
        arq = open(caminho_arq, &#39;w&#39;)
    except PermissionError:
        print(f&#34;\nErro ao salvar os tokens anotados por arquivo no caminho &#39;{caminho}&#39;. Permissão de escrita negada!\n&#34;)
        exit(PERMISSION_ERROR)

    for a, lst_tokens_por_tag_linha in tokens_por_arquivo.items():
        arq.write(f&#34;# Arquivo: {a}&#34;)

        for tokens_por_tag_linha in lst_tokens_por_tag_linha:
            for tag, tokens in tokens_por_tag_linha.items():
                arq.write(f&#34;\n\n - Tag {tag}:\n&#34;)

                for t in tokens:
                    if t[1][1][:2] == &#34;B-&#34;:
                        arq.write(f&#34;\n   {t[0]} -&gt; {t[1][0]}&#34;)
                    else:
                        arq.write(f&#34; {t[1][0]}&#34;)

            arq.write(&#34;\n\n&#34;)

    arq.close()

    # Gera o arquivo com os exemplos de entidades por tag
    arq = None
    caminho_arq = os.path.join(caminho, &#34;exemplos_de_entidades_por_tag.txt&#34;)

    try:
        arq = open(caminho_arq, &#39;w&#39;)
    except PermissionError:
        print(f&#34;\nErro ao salvar os exemplos de entidades no caminho &#39;{caminho}&#39;. Permissão de escrita negada!\n&#34;)
        exit(PERMISSION_ERROR)

    for tag, lst_entidades_por_tag in exemplos_entidades_por_tag.items():
        arq.write(f&#34;- Label {tag}:\n\n&#34;)

        entidades_aninhadas = []
        ent_aninhada = &#39;&#39;

        for e in lst_entidades_por_tag:
            if e[1] == &#34;B-&#34;:
                if ent_aninhada != &#39;&#39;:
                    entidades_aninhadas.append(ent_aninhada)

                ent_aninhada = e[0]
            else:
                ent_aninhada += f&#34; {e[0]}&#34;

        # Inclui a última entidade aninhada apurada
        if ent_aninhada != &#39;&#39;:
            entidades_aninhadas.append(ent_aninhada)

        entidades_aninhadas = list(set(entidades_aninhadas))
        entidades_aninhadas.sort()

        for e in entidades_aninhadas:
            arq.write(f&#34;{e}\n&#34;)

        arq.write(&#34;\n\n&#34;)

    arq.close()

    print(&#34;\n\n **** Sumarização dos tipos de tags e total de entidades de cada uma ****\n&#34;)
    for tag, total in total_tags.items():
        print(f&#34;\t                 {tag} = {total}&#34;)


def converter_jsonl_conll(retirar_sentencas_semelhantes=False, escopo_global_sentencas=True, tamanho_dataset_teste=0.0,
                          concatenar_arquivos=False):
    &#34;&#34;&#34;
    Converte as sentenças anotadas no formato JSONL para o formato CONLL e grava num arquivo de dataset

        @param retirar_sentencas_semelhantes: Indica se as sentenças semelhantes devem ser retiradas no momento de
                                              geração dos datasets
        @param escopo_global_sentencas: Escopo para a análise de sentenças. Se True, fará a comparação com todos os
                                        documentos, caso contrário a comparação será somente no próprio documento
        @param tamanho_dataset_teste: Tamanho do dataset de teste. Obs.: O dataset de teste será subdivido em teste e
                                      validação: 50% teste e 50% validação
        @param concatenar_arquivos: Indica se os arquivos JSONL serão concatenados e ordenados
    &#34;&#34;&#34;
    # Obtém o caminho do arquivo de configuração para montar o caminho completo
    caminho_arquivo_configuracao = REN_CAMINHO_ARQ_CONF
    nome_arquivo_configuracao = &#39;param_ren.conf&#39;
    arq_conf = os.path.join(caminho_arquivo_configuracao, nome_arquivo_configuracao)

    conf = inicializar_parametros(&#39;ren&#39;, arq_conf)

    # Obtém os parâmetros de configuração do módulo
    parametros = {&#39;caminho_arq_conf&#39;: arq_conf,
                  &#39;p_caminho_arq_sentencas&#39;: conf.obter_valor_parametro(&#39;p_caminho_arq_sentencas&#39;),
                  &#39;p_caminho_sentencas_base&#39;: conf.obter_valor_parametro(&#39;p_caminho_sentencas_base&#39;)}

    validar_pastas(parametros)

    caminho = parametros[&#39;p_caminho_arq_sentencas&#39;]
    sentencas_com_tags = []
    entidades_por_arquivo = {}  # Guarda as entidades por arquivo. Objetivo: de fazer uma contabilidade das entidades

    if concatenar_arquivos:
        concatenar_arquivos_jsonl(caminho)
        chave_busca = &#34;concatenado.jsonl&#34;
    else:
        chave_busca = &#34;admin.jsonl&#34;

    # obtém os caminhos dos arquivos recursivamente e os ordena
    arquivos = [os.path.join(pasta, a) for pasta, subpastas, arquivos in os.walk(caminho) for a in arquivos]
    arquivos.sort()

    qtd_arquivos_encontrados = 0
    qtd_arquivos_processados = 0

    print(&#34;\n                  ********* Convertendo sentenças anotadas para o formato CONLL *********\n&#34;)

    # Guarda as sentenças por arquivo, utilizando o nome do arquivo como chave
    arquivos_sentencas = {}

    print(&#34;=&gt; Carga dos arquivos:\n&#34;)

    for a in arquivos:
        if chave_busca in a:
            qtd_arquivos_encontrados += 1
            arquivos_sentencas[a] = carregar_jsonl(a)

    if retirar_sentencas_semelhantes:
        arquivos_sentencas = \
            retirar_sentencas_similares(arquivos_sentencas, parametros[&#39;p_caminho_sentencas_base&#39;],
                                        &#34;SentencasBaseCONLL.pkl&#34;, parametros[&#39;p_caminho_arq_sentencas&#39;],
                                        reprocessar=True, escopo_global_sentencas=escopo_global_sentencas, limiar=0.90)

    print(&#34;\n=&gt; Processamento dos arquivos:\n&#34;)

    for a, sentencas_json in arquivos_sentencas.items():
        encontrou_campos = False
        print(f&#34;Processando o arquivo &#39;{a}&#39; &#34;, end=&#39;&#39;)

        if sentencas_json:
            entidades_por_arquivo_linha = {}  # Guarda as entidades do arquivo organizadas por linha
            qtd_sentencas_validas = 0

            for sj in sentencas_json:
                if &#39;data&#39; in sj[1] and &#39;label&#39; in sj[1] and &#39;id&#39; in sj[1]:
                    encontrou_campos = True
                    qtd_sentencas_validas += 1
                    sentenca_fatiada = fatiar_sentenca(sj[1][&#39;data&#39;], sj[1][&#39;label&#39;])
                    entidades = distribuir_tags(sentenca_fatiada)
                    sentencas_com_tags.append(entidades)
                    chave = f&#34;Linha: {sj[0]}; ID: {sj[1][&#39;id&#39;]}&#34;
                    entidades_por_arquivo_linha[chave] = entidades

            if encontrou_campos:
                entidades_por_arquivo[a] = entidades_por_arquivo_linha
                qtd_arquivos_processados += 1
                print(f&#34;(Qtd. sentenças com anotações válidas = {qtd_sentencas_validas}) -&gt; OK!&#34;)
            else:
                print(f&#34;Falhou: Alguma(s) das chaves &#39;id&#39;, data&#39; e &#39;label&#39; não foram encontradas no arquivo!) -&gt; &#34;
                      f&#34;ERRO!&#34;)
        else:
            print(&#34;) -&gt; ERRO!&#34;)

    if qtd_arquivos_processados &gt; 0:
        gerar_arquivo_conll(sentencas_com_tags, caminho, tamanho_dataset_teste=tamanho_dataset_teste)
        gerar_estatisticas(qtd_arquivos_processados, sentencas_com_tags)
        contabilizar_entidades(caminho, entidades_por_arquivo)

        # Gera um arquivo com os parâmetros de processamento que foram utilizados na geração dos datasets
        arq_parametros_utilizados = None
        nome_arq_parametros_utilizados = os.path.join(caminho, &#39;parametros_utilizados.txt&#39;)

        try:
            arq_parametros_utilizados = open(nome_arq_parametros_utilizados, &#39;w&#39;)
        except PermissionError:
            print(f&#34;\nErro ao criar o arquivo com os parâmetros utilizados na geração dos arquivos CONLL. Permissão de &#34;
                  f&#34;gravação negada na pasta &#39;{caminho}&#39;!\nProcessamento abortado.&#34;)
            exit(PERMISSION_ERROR)

        arq_parametros_utilizados.write(f&#34;# Parâmetros utilizados para geração dos arquivos CONLL:\n&#34;
                                        f&#34;\nPasta destino: &#39;{caminho}&#39;&#34;
                                        f&#34;\nRetirar sentenças semelhantes: {retirar_sentencas_semelhantes}&#34;
                                        f&#34;\nEscopo global de sentenças: {escopo_global_sentencas}&#34;
                                        f&#34;\nLimiar para definição de sentenças similares: 0.9&#34;
                                        f&#34;\nTamanho do dataset de teste: {tamanho_dataset_teste}&#34;
                                        f&#34;\nConcatenar arquivos: {concatenar_arquivos}&#34;)
        arq_parametros_utilizados.close()
        print(f&#34;\n - Os arquivos foram gerados na pasta &#39;{caminho}&#39;\n&#34;)

    if qtd_arquivos_encontrados &gt; 0:
        if qtd_arquivos_encontrados != qtd_arquivos_processados:
            print(&#34;\n\n                 *** ATENÇÃO ***\n&#34;)
            print(&#34;-&gt; Nem todos os arquivos encontrados foram processados. Favor verificar!\n&#34;)
            print(f&#34;   Qtd. de arquivos encontrados..: {qtd_arquivos_encontrados}&#34;)
            print(f&#34;   Qtd. de arquivos processados..: {qtd_arquivos_processados}&#34;)
            print(f&#34;   Qtd. de arquivos com erro.....: {qtd_arquivos_encontrados - qtd_arquivos_processados}&#34;)
    else:
        print(f&#34;\nNão foram encontrados arquivos &#39;{chave_busca}&#39; no caminho &#39;{caminho}&#39;!&#34;)


# Obs.: para rodar este script diretamente no caminho dele, tem que configurar a variável PYTHONPATH com o caminho do
# projeto. Exemplo no Linux: export PYTHONPATH=&#39;/dados/develop/PycharmProjects/mestrado&#39;
if __name__ == &#34;__main__&#34;:
    converter_jsonl_conll(retirar_sentencas_semelhantes=False, escopo_global_sentencas=True, tamanho_dataset_teste=0.0,
                          concatenar_arquivos=False)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="conversor_jsonl_conll.carregar_arquivo"><code class="name flex">
<span>def <span class="ident">carregar_arquivo</span></span>(<span>caminho)</span>
</code></dt>
<dd>
<div class="desc"><p>Carrega as linhas de um arquivo no formato JSONL (Arquivo com sentenças anotadas e exportadas através do Doccano)</p>
<pre><code>@param caminho: Caminho do aquivo que será carregado
@return: Linhas lidas e codificação do arquivo
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def carregar_arquivo(caminho):
    &#34;&#34;&#34;
    Carrega as linhas de um arquivo no formato JSONL (Arquivo com sentenças anotadas e exportadas através do Doccano)

        @param caminho: Caminho do aquivo que será carregado
        @return: Linhas lidas e codificação do arquivo
    &#34;&#34;&#34;
    # Verifica qual a codificação do arquivo
    try:
        codificacao = magic.Magic(mime_encoding=True).from_file(caminho)
    except PermissionError:
        codificacao = &#39;utf-8&#39;  # Assume que a codificação será &#39;utf-8&#39;

    arq_json = None

    try:
        arq_json = open(caminho, &#39;r&#39;, encoding=codificacao)
    except PermissionError:
        print(f&#34;Erro ao ler o arquivo &#39;{caminho}&#39;. Permissão de leitura negada!&#34;)
        return [], None
    except FileNotFoundError:
        print(f&#34;\nErro ao ler o arquivo &#39;{caminho}&#39;. Arquivo não encontrado!\n&#34;)
        exit(FILE_NOT_FOUND_ERROR)
    except LookupError:
        print(f&#34;Falhou: A Codificação &#39;{codificacao}&#39; não é suportada!&#34;)
        return [], None

    linhas = []

    try:
        linhas = arq_json.readlines()
    except UnicodeDecodeError as e:
        print(f&#34;Falha ao ler as linhas do arquivo. Erro: {e}&#34;)

    if arq_json:
        arq_json.close()

    return linhas, codificacao</code></pre>
</details>
</dd>
<dt id="conversor_jsonl_conll.carregar_jsonl"><code class="name flex">
<span>def <span class="ident">carregar_jsonl</span></span>(<span>caminho)</span>
</code></dt>
<dd>
<div class="desc"><p>Carrega os dados de um arquivo no formato JSONL (Arquivo com sentenças anotadas e exportadas através do Doccano)</p>
<pre><code>@param caminho: Caminho do aquivo que será carregado
@return: Lista com tuplas com número da linha da sentença e dicionário contendo a sentença, marcação das
entidades e tags, de cada uma das sentenças do arquivo
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def carregar_jsonl(caminho):
    &#34;&#34;&#34;
    Carrega os dados de um arquivo no formato JSONL (Arquivo com sentenças anotadas e exportadas através do Doccano)

        @param caminho: Caminho do aquivo que será carregado
        @return: Lista com tuplas com número da linha da sentença e dicionário contendo a sentença, marcação das
        entidades e tags, de cada uma das sentenças do arquivo
    &#34;&#34;&#34;
    # Verifica qual a codificação do arquivo
    try:
        codificacao = magic.Magic(mime_encoding=True).from_file(caminho)
    except PermissionError:
        codificacao = &#39;utf-8&#39;  # Assume que a codificação será &#39;utf-8&#39;

    print(f&#34;Carregando o arquivo &#39;{caminho}&#39; (Codificação=&#39;{codificacao}&#39;)&#34;, end=&#39;&#39;)

    arq_json = None

    try:
        arq_json = open(caminho, &#39;r&#39;, encoding=codificacao)
    except PermissionError:
        print(f&#34;(Erro ao ler o arquivo &#39;{caminho}&#39;. Permissão de leitura negada!&#34;, end=&#39;&#39;)
        return []
    except FileNotFoundError:
        print(f&#34;\nErro ao ler o arquivo &#39;{caminho}&#39;. Arquivo não encontrado!\n&#34;)
        exit(FILE_NOT_FOUND_ERROR)
    except LookupError:
        print(f&#34;(Falhou: A Codificação &#39;{codificacao}&#39; não é suportada!&#34;, end=&#39;&#39;)
        return []

    sentencas = []
    linhas = []

    try:
        linhas = arq_json.readlines()
    except UnicodeDecodeError as e:
        print(f&#34;Falhou: {e}&#34;, end=&#39;&#39;)

    for i in range(len(linhas)):
        try:
            sentencas.append((i+1, json.loads(linhas[i].strip(&#39;\n&#39;))))  # Guarda também o número da linha
        except json.decoder.JSONDecodeError as e:
            print(f&#34;Falhou. JSONL mal formado: {e}&#34;, end=&#39;&#39;)

    if arq_json:
        arq_json.close()

    print(&#34; -&gt; OK&#34;)

    return sentencas</code></pre>
</details>
</dd>
<dt id="conversor_jsonl_conll.concatenar_arquivos_jsonl"><code class="name flex">
<span>def <span class="ident">concatenar_arquivos_jsonl</span></span>(<span>caminho)</span>
</code></dt>
<dd>
<div class="desc"><p>Concatena os arquivos JSON 'admin.jsonl' e 'unknown.jsonl' ordenando as linhas</p>
<pre><code>@param caminho: Caminho dos aquivos que serão verificados e concatenados se necessário
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def concatenar_arquivos_jsonl(caminho):
    &#34;&#34;&#34;
    Concatena os arquivos JSON &#39;admin.jsonl&#39; e &#39;unknown.jsonl&#39; ordenando as linhas

        @param caminho: Caminho dos aquivos que serão verificados e concatenados se necessário
    &#34;&#34;&#34;
    print(&#34;\n=&gt; Concatenando e ordenando as linhas dos arquivos JSONL:\n&#34;)

    # obtém os caminhos das pastas recursivamente e os ordena
    caminhos_pastas = [pasta for pasta, subpastas, arquivos in os.walk(caminho)]
    caminhos_pastas.sort()

    arquivos_concatenar = []  # Guarda os arquivos que serão verificados e concatenados se necessário

    # Apura quais arquivos serão concatenados
    for pasta in caminhos_pastas:
        arq_admin = os.path.join(pasta, &#34;admin.jsonl&#34;)
        arq_unknown = os.path.join(pasta, &#34;unknown.jsonl&#34;)

        # Trata o caso de existir somente o arquivo &#34;admin.jsonl&#34; ou &#34;unknown.jsonl&#34;
        if os.path.exists(arq_admin) and os.path.exists(arq_unknown):
            arquivos_concatenar.append((pasta, arq_admin, arq_unknown))
        elif os.path.exists(arq_admin) and not os.path.exists(arq_unknown):
            arquivos_concatenar.append((pasta, arq_admin))
        elif not os.path.exists(arq_admin) and os.path.exists(arq_unknown):
            arquivos_concatenar.append((pasta, arq_unknown))

    # Concatena os arquivos
    for a in arquivos_concatenar:
        codificacao = None
        arq_admin_lido = None
        arq_unknown_lido = None
        arq_sozinho_lido = None  # Guardará ou o arquivo &#39;admin.jsonl&#39; ou &#39;unknown.jsonl&#39;, caso tenha somente um deles

        if len(a) == 3:
            print(f&#34;Concatenando e ordenando os arquivos: &#39;{a[1]}&#39; e &#39;{a[2]}&#39;...&#34;, end=&#39;&#39;, flush=True)
            arq_admin_lido, codificacao = carregar_arquivo(a[1])  # Assume que a codificação dos dois arquivos é a mesma
            arq_unknown_lido, _ = carregar_arquivo(a[2])

            if not arq_admin_lido or not arq_unknown_lido:
                print(&#34;Erro no carregamento das linhas. Verifique o conteúdo dos arquivos!&#34;)
                continue
        elif len(a) == 2:
            print(f&#34;Somente carregando o arquivo &#39;{a[1]}&#39; pois só existe ele...&#34;, end=&#39;&#39;, flush=True)
            arq_sozinho_lido, codificacao = carregar_arquivo(a[1])

            if not arq_sozinho_lido:
                print(f&#34;Erro no carregamento das linhas. Verifique o conteúdo do arquivo!&#34;)
                continue

        linhas_ordenadas = []

        if arq_admin_lido:
            linhas_ordenadas = concatenar_ordenar_linhas(arq_admin_lido, arq_unknown_lido)
        elif arq_sozinho_lido:
            linhas_ordenadas = arq_sozinho_lido

        if linhas_ordenadas:
            caminho_arq_concatenado = os.path.join(a[0], &#34;concatenado.jsonl&#34;)
            arq_json_concatenado = None

            try:
                arq_json_concatenado = open(caminho_arq_concatenado, &#39;w&#39;, encoding=codificacao)
            except PermissionError:
                print(f&#34;Erro ao criar o arquivo &#39;{caminho_arq_concatenado}&#39;. Permissão de escrita negada!\n&#34;)
                exit(PERMISSION_ERROR)
            except LookupError:
                print(f&#34;Falhou: A Codificação &#39;{codificacao}&#39; não é suportada!&#34;)
                continue

            arq_json_concatenado.writelines(linhas_ordenadas)
            arq_json_concatenado.close()
            print(&#34; OK!&#34;)</code></pre>
</details>
</dd>
<dt id="conversor_jsonl_conll.concatenar_ordenar_linhas"><code class="name flex">
<span>def <span class="ident">concatenar_ordenar_linhas</span></span>(<span>arq_1, arq_2)</span>
</code></dt>
<dd>
<div class="desc"><p>Concatena e ordena as linhas de dois arquivos JSONL</p>
<pre><code>@param arq_1: linhas lidas do primeiro arquivo
@param arq_2: linhas lidas do segundo arquivo
@return: Lista com as linhas dos dois arquivos concatenadas e ordenadas
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def concatenar_ordenar_linhas(arq_1, arq_2):
    &#34;&#34;&#34;
    Concatena e ordena as linhas de dois arquivos JSONL

        @param arq_1: linhas lidas do primeiro arquivo
        @param arq_2: linhas lidas do segundo arquivo
        @return: Lista com as linhas dos dois arquivos concatenadas e ordenadas
    &#34;&#34;&#34;
    linhas_juntas = []

    # Obtém os IDs de cada linha do arquivo e cria tuplas auxiliares para facilitar a ordenação das linhas pelo ID
    for arq in [arq_1, arq_2]:
        for linha in arq:
            linhas_juntas.append((int(linha.split(&#34;,&#34;)[0].split(&#34;:&#34;)[1]), linha))

    linhas_juntas.sort()

    # Retira os IDs aulixiares e obtém as linhas ordenadas
    linhas_juntas_ordenadas = [linha[1] for linha in linhas_juntas]

    return linhas_juntas_ordenadas</code></pre>
</details>
</dd>
<dt id="conversor_jsonl_conll.contabilizar_entidades"><code class="name flex">
<span>def <span class="ident">contabilizar_entidades</span></span>(<span>caminho, entidades_por_arquivo)</span>
</code></dt>
<dd>
<div class="desc"><p>Contabiliza as entidades constantes nos arquivos. Mostra sumário na tela, com o total de tokens por entidade,
grava um arquivo com o total de cada tipo de tag encontrada nos arquivos e grava um outro arquivo com os exemplos
de entidades organizados por tag</p>
<pre><code>@param caminho: Caminho onde o arquivo com os detalhes será gravado
@param entidades_por_arquivo: Dicionário cuja chave é o nome do arquivo e o conteúdo é outro dicionário cuja
                              chave é a linha+id e o conteúdo uma lista de tuplas de tokens e suas respectivas
                              tags
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def contabilizar_entidades(caminho, entidades_por_arquivo):
    &#34;&#34;&#34;
    Contabiliza as entidades constantes nos arquivos. Mostra sumário na tela, com o total de tokens por entidade,
    grava um arquivo com o total de cada tipo de tag encontrada nos arquivos e grava um outro arquivo com os exemplos
    de entidades organizados por tag

        @param caminho: Caminho onde o arquivo com os detalhes será gravado
        @param entidades_por_arquivo: Dicionário cuja chave é o nome do arquivo e o conteúdo é outro dicionário cuja
                                      chave é a linha+id e o conteúdo uma lista de tuplas de tokens e suas respectivas
                                      tags
    &#34;&#34;&#34;
    tokens_por_arquivo = {}  # Guarda os tokens diferentes de &#39;O&#39; encontrados em cada arquivo. Organiza por arquivo.
    total_tags = {}  # Guarda o total de cada tipo de tag encontrada nos arquivos. Organiza por nome da tag.
    exemplos_entidades_por_tag = {}  # Guarda os exemplos de entidades por tag

    for a, entidades_por_linha in entidades_por_arquivo.items():
        tokens_por_arquivo[a] = []
        tokens_por_tag_linha = {}

        for linha_id, entidades in entidades_por_linha.items():
            for e in entidades:
                inicio_tag = e[1][:2]
                chave = e[1][2:]  # Pega o nome da tag sem o B-

                if inicio_tag == &#39;B-&#39; or inicio_tag == &#39;I-&#39;:
                    # Guarda os tokens marcados para o arquivo e a referida linha onde eles se encontram
                    if chave in tokens_por_tag_linha:
                        tokens_por_tag_linha[chave].append((linha_id, e))
                    else:
                        tokens_por_tag_linha[chave] = [(linha_id, e)]

                    # Guarda os exemplos por tag
                    if chave in exemplos_entidades_por_tag:
                        exemplos_entidades_por_tag[chave].append((e[0], inicio_tag))
                    else:
                        exemplos_entidades_por_tag[chave] = [(e[0], inicio_tag)]

                # Contabiliza o total de cada tipo de tag
                if inicio_tag == &#39;B-&#39;:
                    if chave in total_tags:
                        total_tags[chave] += 1
                    else:
                        total_tags[chave] = 1

        tokens_por_arquivo[a].append(tokens_por_tag_linha)

    # Gera o arquivo com os dados de tokens por arquivo
    arq = None
    caminho_arq = os.path.join(caminho, &#34;tokens_anotados_por_arquivo.txt&#34;)

    try:
        arq = open(caminho_arq, &#39;w&#39;)
    except PermissionError:
        print(f&#34;\nErro ao salvar os tokens anotados por arquivo no caminho &#39;{caminho}&#39;. Permissão de escrita negada!\n&#34;)
        exit(PERMISSION_ERROR)

    for a, lst_tokens_por_tag_linha in tokens_por_arquivo.items():
        arq.write(f&#34;# Arquivo: {a}&#34;)

        for tokens_por_tag_linha in lst_tokens_por_tag_linha:
            for tag, tokens in tokens_por_tag_linha.items():
                arq.write(f&#34;\n\n - Tag {tag}:\n&#34;)

                for t in tokens:
                    if t[1][1][:2] == &#34;B-&#34;:
                        arq.write(f&#34;\n   {t[0]} -&gt; {t[1][0]}&#34;)
                    else:
                        arq.write(f&#34; {t[1][0]}&#34;)

            arq.write(&#34;\n\n&#34;)

    arq.close()

    # Gera o arquivo com os exemplos de entidades por tag
    arq = None
    caminho_arq = os.path.join(caminho, &#34;exemplos_de_entidades_por_tag.txt&#34;)

    try:
        arq = open(caminho_arq, &#39;w&#39;)
    except PermissionError:
        print(f&#34;\nErro ao salvar os exemplos de entidades no caminho &#39;{caminho}&#39;. Permissão de escrita negada!\n&#34;)
        exit(PERMISSION_ERROR)

    for tag, lst_entidades_por_tag in exemplos_entidades_por_tag.items():
        arq.write(f&#34;- Label {tag}:\n\n&#34;)

        entidades_aninhadas = []
        ent_aninhada = &#39;&#39;

        for e in lst_entidades_por_tag:
            if e[1] == &#34;B-&#34;:
                if ent_aninhada != &#39;&#39;:
                    entidades_aninhadas.append(ent_aninhada)

                ent_aninhada = e[0]
            else:
                ent_aninhada += f&#34; {e[0]}&#34;

        # Inclui a última entidade aninhada apurada
        if ent_aninhada != &#39;&#39;:
            entidades_aninhadas.append(ent_aninhada)

        entidades_aninhadas = list(set(entidades_aninhadas))
        entidades_aninhadas.sort()

        for e in entidades_aninhadas:
            arq.write(f&#34;{e}\n&#34;)

        arq.write(&#34;\n\n&#34;)

    arq.close()

    print(&#34;\n\n **** Sumarização dos tipos de tags e total de entidades de cada uma ****\n&#34;)
    for tag, total in total_tags.items():
        print(f&#34;\t                 {tag} = {total}&#34;)</code></pre>
</details>
</dd>
<dt id="conversor_jsonl_conll.converter_jsonl_conll"><code class="name flex">
<span>def <span class="ident">converter_jsonl_conll</span></span>(<span>retirar_sentencas_semelhantes=False, escopo_global_sentencas=True, tamanho_dataset_teste=0.0, concatenar_arquivos=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Converte as sentenças anotadas no formato JSONL para o formato CONLL e grava num arquivo de dataset</p>
<pre><code>@param retirar_sentencas_semelhantes: Indica se as sentenças semelhantes devem ser retiradas no momento de
                                      geração dos datasets
@param escopo_global_sentencas: Escopo para a análise de sentenças. Se True, fará a comparação com todos os
                                documentos, caso contrário a comparação será somente no próprio documento
@param tamanho_dataset_teste: Tamanho do dataset de teste. Obs.: O dataset de teste será subdivido em teste e
                              validação: 50% teste e 50% validação
@param concatenar_arquivos: Indica se os arquivos JSONL serão concatenados e ordenados
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def converter_jsonl_conll(retirar_sentencas_semelhantes=False, escopo_global_sentencas=True, tamanho_dataset_teste=0.0,
                          concatenar_arquivos=False):
    &#34;&#34;&#34;
    Converte as sentenças anotadas no formato JSONL para o formato CONLL e grava num arquivo de dataset

        @param retirar_sentencas_semelhantes: Indica se as sentenças semelhantes devem ser retiradas no momento de
                                              geração dos datasets
        @param escopo_global_sentencas: Escopo para a análise de sentenças. Se True, fará a comparação com todos os
                                        documentos, caso contrário a comparação será somente no próprio documento
        @param tamanho_dataset_teste: Tamanho do dataset de teste. Obs.: O dataset de teste será subdivido em teste e
                                      validação: 50% teste e 50% validação
        @param concatenar_arquivos: Indica se os arquivos JSONL serão concatenados e ordenados
    &#34;&#34;&#34;
    # Obtém o caminho do arquivo de configuração para montar o caminho completo
    caminho_arquivo_configuracao = REN_CAMINHO_ARQ_CONF
    nome_arquivo_configuracao = &#39;param_ren.conf&#39;
    arq_conf = os.path.join(caminho_arquivo_configuracao, nome_arquivo_configuracao)

    conf = inicializar_parametros(&#39;ren&#39;, arq_conf)

    # Obtém os parâmetros de configuração do módulo
    parametros = {&#39;caminho_arq_conf&#39;: arq_conf,
                  &#39;p_caminho_arq_sentencas&#39;: conf.obter_valor_parametro(&#39;p_caminho_arq_sentencas&#39;),
                  &#39;p_caminho_sentencas_base&#39;: conf.obter_valor_parametro(&#39;p_caminho_sentencas_base&#39;)}

    validar_pastas(parametros)

    caminho = parametros[&#39;p_caminho_arq_sentencas&#39;]
    sentencas_com_tags = []
    entidades_por_arquivo = {}  # Guarda as entidades por arquivo. Objetivo: de fazer uma contabilidade das entidades

    if concatenar_arquivos:
        concatenar_arquivos_jsonl(caminho)
        chave_busca = &#34;concatenado.jsonl&#34;
    else:
        chave_busca = &#34;admin.jsonl&#34;

    # obtém os caminhos dos arquivos recursivamente e os ordena
    arquivos = [os.path.join(pasta, a) for pasta, subpastas, arquivos in os.walk(caminho) for a in arquivos]
    arquivos.sort()

    qtd_arquivos_encontrados = 0
    qtd_arquivos_processados = 0

    print(&#34;\n                  ********* Convertendo sentenças anotadas para o formato CONLL *********\n&#34;)

    # Guarda as sentenças por arquivo, utilizando o nome do arquivo como chave
    arquivos_sentencas = {}

    print(&#34;=&gt; Carga dos arquivos:\n&#34;)

    for a in arquivos:
        if chave_busca in a:
            qtd_arquivos_encontrados += 1
            arquivos_sentencas[a] = carregar_jsonl(a)

    if retirar_sentencas_semelhantes:
        arquivos_sentencas = \
            retirar_sentencas_similares(arquivos_sentencas, parametros[&#39;p_caminho_sentencas_base&#39;],
                                        &#34;SentencasBaseCONLL.pkl&#34;, parametros[&#39;p_caminho_arq_sentencas&#39;],
                                        reprocessar=True, escopo_global_sentencas=escopo_global_sentencas, limiar=0.90)

    print(&#34;\n=&gt; Processamento dos arquivos:\n&#34;)

    for a, sentencas_json in arquivos_sentencas.items():
        encontrou_campos = False
        print(f&#34;Processando o arquivo &#39;{a}&#39; &#34;, end=&#39;&#39;)

        if sentencas_json:
            entidades_por_arquivo_linha = {}  # Guarda as entidades do arquivo organizadas por linha
            qtd_sentencas_validas = 0

            for sj in sentencas_json:
                if &#39;data&#39; in sj[1] and &#39;label&#39; in sj[1] and &#39;id&#39; in sj[1]:
                    encontrou_campos = True
                    qtd_sentencas_validas += 1
                    sentenca_fatiada = fatiar_sentenca(sj[1][&#39;data&#39;], sj[1][&#39;label&#39;])
                    entidades = distribuir_tags(sentenca_fatiada)
                    sentencas_com_tags.append(entidades)
                    chave = f&#34;Linha: {sj[0]}; ID: {sj[1][&#39;id&#39;]}&#34;
                    entidades_por_arquivo_linha[chave] = entidades

            if encontrou_campos:
                entidades_por_arquivo[a] = entidades_por_arquivo_linha
                qtd_arquivos_processados += 1
                print(f&#34;(Qtd. sentenças com anotações válidas = {qtd_sentencas_validas}) -&gt; OK!&#34;)
            else:
                print(f&#34;Falhou: Alguma(s) das chaves &#39;id&#39;, data&#39; e &#39;label&#39; não foram encontradas no arquivo!) -&gt; &#34;
                      f&#34;ERRO!&#34;)
        else:
            print(&#34;) -&gt; ERRO!&#34;)

    if qtd_arquivos_processados &gt; 0:
        gerar_arquivo_conll(sentencas_com_tags, caminho, tamanho_dataset_teste=tamanho_dataset_teste)
        gerar_estatisticas(qtd_arquivos_processados, sentencas_com_tags)
        contabilizar_entidades(caminho, entidades_por_arquivo)

        # Gera um arquivo com os parâmetros de processamento que foram utilizados na geração dos datasets
        arq_parametros_utilizados = None
        nome_arq_parametros_utilizados = os.path.join(caminho, &#39;parametros_utilizados.txt&#39;)

        try:
            arq_parametros_utilizados = open(nome_arq_parametros_utilizados, &#39;w&#39;)
        except PermissionError:
            print(f&#34;\nErro ao criar o arquivo com os parâmetros utilizados na geração dos arquivos CONLL. Permissão de &#34;
                  f&#34;gravação negada na pasta &#39;{caminho}&#39;!\nProcessamento abortado.&#34;)
            exit(PERMISSION_ERROR)

        arq_parametros_utilizados.write(f&#34;# Parâmetros utilizados para geração dos arquivos CONLL:\n&#34;
                                        f&#34;\nPasta destino: &#39;{caminho}&#39;&#34;
                                        f&#34;\nRetirar sentenças semelhantes: {retirar_sentencas_semelhantes}&#34;
                                        f&#34;\nEscopo global de sentenças: {escopo_global_sentencas}&#34;
                                        f&#34;\nLimiar para definição de sentenças similares: 0.9&#34;
                                        f&#34;\nTamanho do dataset de teste: {tamanho_dataset_teste}&#34;
                                        f&#34;\nConcatenar arquivos: {concatenar_arquivos}&#34;)
        arq_parametros_utilizados.close()
        print(f&#34;\n - Os arquivos foram gerados na pasta &#39;{caminho}&#39;\n&#34;)

    if qtd_arquivos_encontrados &gt; 0:
        if qtd_arquivos_encontrados != qtd_arquivos_processados:
            print(&#34;\n\n                 *** ATENÇÃO ***\n&#34;)
            print(&#34;-&gt; Nem todos os arquivos encontrados foram processados. Favor verificar!\n&#34;)
            print(f&#34;   Qtd. de arquivos encontrados..: {qtd_arquivos_encontrados}&#34;)
            print(f&#34;   Qtd. de arquivos processados..: {qtd_arquivos_processados}&#34;)
            print(f&#34;   Qtd. de arquivos com erro.....: {qtd_arquivos_encontrados - qtd_arquivos_processados}&#34;)
    else:
        print(f&#34;\nNão foram encontrados arquivos &#39;{chave_busca}&#39; no caminho &#39;{caminho}&#39;!&#34;)</code></pre>
</details>
</dd>
<dt id="conversor_jsonl_conll.distribuir_tags"><code class="name flex">
<span>def <span class="ident">distribuir_tags</span></span>(<span>sentenca_fatiada)</span>
</code></dt>
<dd>
<div class="desc"><p>Distribui as tags entre os tokens da sentença fatiada e, caso for necessário, aninha as tags de entidades que
possuem mais de um token</p>
<pre><code>@param sentenca_fatiada: Lista contendo tuplas de fatias da sentença com suas respectivas tags
@return: Lista contendo tuplas de entidades com suas respectivas tags
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def distribuir_tags(sentenca_fatiada):
    &#34;&#34;&#34;
    Distribui as tags entre os tokens da sentença fatiada e, caso for necessário, aninha as tags de entidades que
    possuem mais de um token

        @param sentenca_fatiada: Lista contendo tuplas de fatias da sentença com suas respectivas tags
        @return: Lista contendo tuplas de entidades com suas respectivas tags
    &#34;&#34;&#34;
    entidades = []

    for fatia in sentenca_fatiada:
        # Obs.: fatia[0] = fatia da sentença; fatia[1] = tag referente à fatia
        entidade_tokens = fatia[0].split()

        if entidade_tokens:
            tag = &#39;B-&#39; + fatia[1] if fatia[1] != &#39;O&#39; else &#39;O&#39;
            entidades.append((entidade_tokens[0], tag))

            if len(entidade_tokens) &gt; 1:
                tag = &#39;I-&#39; + fatia[1] if fatia[1] != &#39;O&#39; else &#39;O&#39;

                for e in entidade_tokens[1:]:
                    entidades.append((e, tag))

    return entidades</code></pre>
</details>
</dd>
<dt id="conversor_jsonl_conll.fatiar_sentenca"><code class="name flex">
<span>def <span class="ident">fatiar_sentenca</span></span>(<span>sentenca, marcadores_entidades)</span>
</code></dt>
<dd>
<div class="desc"><p>Fatia uma sentença com base nos marcadores dos labels e separas as entidades que receberam label, dos outros tokens</p>
<pre><code>@param sentenca: Sentença que será fatiada
@param marcadores_entidades: Lista contendo listas com os marcadores de entidades
@return: Lista contendo tuplas de fatias da sentença com suas respectivas tags
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fatiar_sentenca(sentenca, marcadores_entidades):
    &#34;&#34;&#34;
    Fatia uma sentença com base nos marcadores dos labels e separas as entidades que receberam label, dos outros tokens

        @param sentenca: Sentença que será fatiada
        @param marcadores_entidades: Lista contendo listas com os marcadores de entidades
        @return: Lista contendo tuplas de fatias da sentença com suas respectivas tags
    &#34;&#34;&#34;
    # Prepara um dicionário com os marcadores para auxiliar no fatiamento. Ficará com o formato conforme exemplo:
    # - Marcador original: [[0, 6, &#39;Organizacao&#39;], [69, 79, &#39;Local&#39;]]
    # - Marcador auxiliar: {&#39;0-6&#39;: &#39;Organizacao&#39;, &#39;69-79&#39;: &#39;Local&#39;}
    dict_marcadores_entidades = {str(m[0]) + &#39;-&#39; + str(m[1]): m[2] for m in marcadores_entidades}

    # Cria uma lista com os índices dos marcadores para auxiliar no fatiamento. Ficará com o formato conforme exemplo:
    # - Marcador original: [[0, 6, &#39;Organizacao&#39;], [69, 79, &#39;Local&#39;]]
    # - lista auxiliar com os índices: [0, 0, 6, 69, 79, None]. Obs.: Inclusão do 0 e do None para ajudar no fatiamento
    lista_aux = [0]

    for m in marcadores_entidades:
        lista_aux.append(m[0])
        lista_aux.append(m[1])

    # Ordena a lista para evitar erros no fatiamento, pois o Doccano pode exportar a lista de marcadores fora de ordem.
    # Exemplo real de export da lista de marcadores fora de ordem:
    # {&#39;id&#39;: 78, &#39;data&#39;: &#39;14h em 12/09/17 Página 2&#39;, &#39;label&#39;: [[7, 15, &#39;Tempo&#39;], [0, 3, &#39;Tempo&#39;]]}
    # Se utilizar a lista auxiliar com os marcadores fora de ordem, o fatiamento ficará errado!
    lista_aux.sort()

    lista_aux.append(None)

    # Fatia as sentenças e atribui os labels a cada token
    sentenca_fatiada = []

    # Percorre a lista axiliar e pega os marcadores de 2 em 2 (i e i+1) e utiliza como início (i) e fim (i+1) para
    # fatiar sentença e obter as entidades (obs.: Ainda não serão tokenizadas) e atribuir as tags correspondentes.
    # A tag &#39;O&#39; não é informada pelo Doccano, ela é atribuida aqui nesta função para as entidades que não foram nomeadas
    for i in range(len(lista_aux) - 1):
        inicio = lista_aux[i]
        fim = lista_aux[i + 1]
        entidade = sentenca[inicio:fim]
        chave = str(inicio) + &#39;-&#39; + str(fim)
        tag = dict_marcadores_entidades[chave].upper() if chave in dict_marcadores_entidades else &#39;O&#39;

        # Evita fatias vazias:
        # - no caso de inicio e fim iguais (0,0): Se fatiar assim vai retornar uma string vazia;
        # - caso a sentença termine com uma entidade diferente de &#39;O&#39;, exemplo: &#34;hoje é dia 20/01/2022&#34;, nesta sentença
        #   a última entidade é uma data e o marcador de fim estará apontado para 21 (tamanho da sentença), logo, fatiar
        #   a partir da posição 21 retornará uma string vazia.
        if entidade != &#39;&#39;:
            sentenca_fatiada.append((entidade, tag))

    return sentenca_fatiada</code></pre>
</details>
</dd>
<dt id="conversor_jsonl_conll.gerar_arquivo_conll"><code class="name flex">
<span>def <span class="ident">gerar_arquivo_conll</span></span>(<span>sentencas_com_tags, pasta_destino_dataset, tamanho_dataset_teste=0.0)</span>
</code></dt>
<dd>
<div class="desc"><p>Gera um arquivo (ou mais, caso seja especificado o tamanho do dataset de teste) no formato CONLL</p>
<pre><code>@param sentencas_com_tags: Lista contendo listas de tuplas com tokens e suas respectivas tags. Cada uma dessas
                           listas de tuplas representa uma sentença.
@param pasta_destino_dataset: Pasta onde será gerado o arquivo CONLL
@param tamanho_dataset_teste: Tamanho do dataset de teste. Obs.: O dataset de teste será subdivido em teste e
                              validação: 50% teste e 50% validação
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gerar_arquivo_conll(sentencas_com_tags, pasta_destino_dataset, tamanho_dataset_teste=0.0):
    &#34;&#34;&#34;
    Gera um arquivo (ou mais, caso seja especificado o tamanho do dataset de teste) no formato CONLL

        @param sentencas_com_tags: Lista contendo listas de tuplas com tokens e suas respectivas tags. Cada uma dessas
                                   listas de tuplas representa uma sentença.
        @param pasta_destino_dataset: Pasta onde será gerado o arquivo CONLL
        @param tamanho_dataset_teste: Tamanho do dataset de teste. Obs.: O dataset de teste será subdivido em teste e
                                      validação: 50% teste e 50% validação
    &#34;&#34;&#34;
    print(&#34;\n=&gt; Gerando arquivo(s) no formato CONLL... &#34;, end=&#39;&#39;)

    datasets = {}

    if tamanho_dataset_teste == 0.0:
        datasets = {&#34;dataset_ner.conll&#34;: sentencas_com_tags}
    elif tamanho_dataset_teste &gt; 0.0:
        # Divide e prepara os datasets para a geração dos arquivos CONLL
        treino = None
        teste = None
        validacao = None

        try:
            treino, teste = \
                train_test_split(sentencas_com_tags, test_size=tamanho_dataset_teste, shuffle=False, random_state=44)
            teste, validacao = train_test_split(teste, test_size=0.5, shuffle=False, random_state=44)
        except ValueError as e:
            print(f&#34;-&gt; ERRO: Arquivos CONLL não gerados. Não foi possível dividir o dataset. Mensagem do &#34;
                  f&#34;&#39;train_test_split&#39;: {e}&#34;)
            exit(VALUE_ERROR)

        datasets = {&#34;dataset_ner_train.conll&#34;: treino, &#34;dataset_ner_test.conll&#34;: teste,
                    &#34;dataset_ner_val.conll&#34;: validacao}

        print(f&#34;\n\nATENÇÃO: O dataset foi dividido (tamanho_dataset_teste={tamanho_dataset_teste}) e as partes ficaram&#34;
              f&#34; com as seguintes quantidades de sentenças:\n\n - Treino = {len(treino)}\n - Teste = {len(teste)}&#34;
              f&#34;\n - Validação = {len(validacao)}\n&#34;)
    else:
        print(f&#34;-&gt; ERRO: Arquivos CONLL não gerados. O tamanho &#39;{tamanho_dataset_teste}&#39; não é válido para o dataset &#34;
              f&#34;de teste!\n&#34;)
        exit(INVALID_CONTENT)

    for nome_arq, sentencas_com_tags in datasets.items():
        caminho_destino_dataset = os.path.join(pasta_destino_dataset, nome_arq)
        arq_dataset = None

        try:
            arq_dataset = open(caminho_destino_dataset, &#39;w&#39;)
        except PermissionError:
            print(f&#34;\nErro ao criar o arquivo de dataset na pasta &#39;{pasta_destino_dataset}&#39;. Permissão de gravação &#34;
                  f&#34;negada!\n&#34;)
            exit(PERMISSION_ERROR)

        for sentenca in sentencas_com_tags:
            for token in sentenca:
                arq_dataset.write(f&#34;{token[0]} {token[1]}\n&#34;)

            arq_dataset.write(&#34;\n&#34;)

        if arq_dataset:
            arq_dataset.close()

    print(&#34;-&gt; OK&#34;)</code></pre>
</details>
</dd>
<dt id="conversor_jsonl_conll.gerar_estatisticas"><code class="name flex">
<span>def <span class="ident">gerar_estatisticas</span></span>(<span>qtd_arquivos_processados, sentencas_com_tags)</span>
</code></dt>
<dd>
<div class="desc"><p>Gera as estatísticas do processo de conversão dos arquivos anotados para o formato CONLL</p>
<pre><code>@param qtd_arquivos_processados: Quantidade de arquivos processados
@param sentencas_com_tags: Lista contendo listas de tuplas com tokens e suas respectivas tags. Cada uma dessas
                           listas de tuplas representa uma sentença.
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gerar_estatisticas(qtd_arquivos_processados, sentencas_com_tags):
    &#34;&#34;&#34;
    Gera as estatísticas do processo de conversão dos arquivos anotados para o formato CONLL

        @param qtd_arquivos_processados: Quantidade de arquivos processados
        @param sentencas_com_tags: Lista contendo listas de tuplas com tokens e suas respectivas tags. Cada uma dessas
                                   listas de tuplas representa uma sentença.
    &#34;&#34;&#34;
    qtd_sentencas = len(sentencas_com_tags)
    total_tokens = 0
    qtd_entidades = 0

    for sentenca in sentencas_com_tags:
        total_tokens += len(sentenca)

        # Contabiliza só as entidades que tem tags começadas com &#39;B-&#39;
        for token in sentenca:
            if &#34;B-&#34; in token[1]:
                qtd_entidades += 1

    # Para evitar a divisão por zero
    if total_tokens == 0:
        print(&#34;\nNão foi possível gerar as estatísticas porque não há tokens!&#34;)
        return

    print(&#34;\n\n =&gt; Estatísticas do processo de conversão dos arquivos anotados\n&#34;)
    print(&#34;\tQuantidade de arquivos processados..................:&#34;, qtd_arquivos_processados)
    print(&#34;\tQuantidade de sentenças.............................:&#34;, qtd_sentencas)
    print(&#34;\tTotal de tokens.....................................:&#34;, total_tokens)
    print(&#34;\tQuantidade de entidades (exceto tag &#39;O&#39;)............:&#34;, qtd_entidades)
    print(f&#34;\tMédia de sentenças por arquivo......................: {(qtd_sentencas/qtd_arquivos_processados):.2f}&#34;)
    print(f&#34;\tMédia de tokens por arquivo.........................: {(total_tokens/qtd_arquivos_processados):.2f}&#34;)
    print(f&#34;\tMédia de tokens por sentença........................: {(total_tokens/qtd_sentencas):.2f}&#34;)
    print(f&#34;\tMédia de entidades (exceto tag &#39;O&#39;) por arquivo.....: {(qtd_entidades/qtd_arquivos_processados):.2f}&#34;)
    print(f&#34;\tMédia de entidades (exceto tag &#39;O&#39;) por sentença....: {(qtd_entidades/qtd_sentencas):.2f}&#34;)
    print(f&#34;\t% de tokens que são entidades (exceto tag &#39;O&#39;)......: {((qtd_entidades/total_tokens) * 100):.2f}%&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="conversor_jsonl_conll.carregar_arquivo" href="#conversor_jsonl_conll.carregar_arquivo">carregar_arquivo</a></code></li>
<li><code><a title="conversor_jsonl_conll.carregar_jsonl" href="#conversor_jsonl_conll.carregar_jsonl">carregar_jsonl</a></code></li>
<li><code><a title="conversor_jsonl_conll.concatenar_arquivos_jsonl" href="#conversor_jsonl_conll.concatenar_arquivos_jsonl">concatenar_arquivos_jsonl</a></code></li>
<li><code><a title="conversor_jsonl_conll.concatenar_ordenar_linhas" href="#conversor_jsonl_conll.concatenar_ordenar_linhas">concatenar_ordenar_linhas</a></code></li>
<li><code><a title="conversor_jsonl_conll.contabilizar_entidades" href="#conversor_jsonl_conll.contabilizar_entidades">contabilizar_entidades</a></code></li>
<li><code><a title="conversor_jsonl_conll.converter_jsonl_conll" href="#conversor_jsonl_conll.converter_jsonl_conll">converter_jsonl_conll</a></code></li>
<li><code><a title="conversor_jsonl_conll.distribuir_tags" href="#conversor_jsonl_conll.distribuir_tags">distribuir_tags</a></code></li>
<li><code><a title="conversor_jsonl_conll.fatiar_sentenca" href="#conversor_jsonl_conll.fatiar_sentenca">fatiar_sentenca</a></code></li>
<li><code><a title="conversor_jsonl_conll.gerar_arquivo_conll" href="#conversor_jsonl_conll.gerar_arquivo_conll">gerar_arquivo_conll</a></code></li>
<li><code><a title="conversor_jsonl_conll.gerar_estatisticas" href="#conversor_jsonl_conll.gerar_estatisticas">gerar_estatisticas</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>